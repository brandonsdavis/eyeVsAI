{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning v1 Image Classification Development\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Design and implement a custom neural network from scratch\n",
    "- Learn fundamental deep learning concepts through hands-on implementation\n",
    "- Compare performance with shallow learning approaches\n",
    "- Establish baseline for deep learning model improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms, datasets\nfrom torchinfo import summary\nimport os\nimport sys\nfrom PIL import Image\nimport pickle\nfrom pathlib import Path\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport time\nfrom typing import Dict, Any, List, Tuple\n\n# Add parent directory to path for imports\nsys.path.append('../..')\nsys.path.append('..')\n\n# Import from extracted modules\nfrom src.classifier import DeepLearningV1Classifier\nfrom src.trainer import DeepLearningV1Trainer\nfrom src.model import DeepLearningV1\nfrom src.config import DeepLearningV1Config\nfrom src.data_loader import create_data_loaders, UnifiedDataset, get_transforms\n\n# Import from ml_models_core\nfrom ml_models_core.src.base_classifier import BaseImageClassifier\nfrom ml_models_core.src.model_registry import ModelRegistry, ModelMetadata\nfrom ml_models_core.src.utils import ModelUtils\nfrom ml_models_core.src.data_loaders import get_unified_classification_data\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Plot settings\nplt.style.use('default')\nsns.set_palette('husl')\n\nprint(\"Setup complete - using extracted modules\")\nprint(\"All required modules imported successfully\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Create configuration for deep learning v1\nconfig = DeepLearningV1Config(\n    image_size=(128, 128),  # Optimal 128x128 input size\n    batch_size=8,\n    learning_rate=0.001,\n    num_epochs=30,\n    patience=5,\n    dropout_rate=0.5,\n    random_seed=42\n)\n\n# Use the correct dataset path for this project  \ndataset_path = \"../../data/downloads/combined_unified_classification\"\n\n# Check if dataset exists\nif not os.path.exists(dataset_path):\n    print(f\"Dataset not found at {dataset_path}\")\n    dataset_path = \"data/downloads/combined_unified_classification\"\n    if not os.path.exists(dataset_path):\n        print(f\"Dataset not found at {dataset_path}\")\n        # Try to use the data manager to get/create the dataset\n        try:\n            from ml_models_core.src.data_manager import get_dataset_manager\n            manager = get_dataset_manager()\n            \n            dataset_path = manager.get_dataset_path('combined_unified_classification')\n            if not dataset_path:\n                print(\"Creating unified classification dataset...\")\n                available_datasets = ['oxford_pets', 'kaggle_vegetables', 'street_foods', 'musical_instruments']\n                dataset_path = manager.create_combined_dataset(\n                    dataset_names=available_datasets,\n                    output_name=\"combined_unified_classification\",\n                    class_mapping=None\n                )\n        except Exception as e:\n            print(f\"Error accessing unified dataset: {e}\")\n            # Fallback to oxford pets\n            dataset_path = \"data/downloads/oxford_pets\"\n\nprint(f\"Using dataset path: {dataset_path}\")\n\n# Create data loaders using extracted modules\ntry:\n    print(\"Creating data loaders using extracted modules...\")\n    train_loader, val_loader, test_loader, class_names = create_data_loaders(dataset_path, config)\n    \n    print(f\"✅ Data loaders created successfully\")\n    print(f\"Found {len(class_names)} classes: {class_names[:5]}{'...' if len(class_names) > 5 else ''}\")\n    print(f\"Training samples: {len(train_loader.dataset)}\")\n    print(f\"Validation samples: {len(val_loader.dataset)}\")\n    print(f\"Test samples: {len(test_loader.dataset)}\")\n    \n    # Test loading a single batch to verify everything works\n    print(f\"\\nTesting data loading with {config.image_size[0]}x{config.image_size[1]} images...\")\n    sample_batch = next(iter(train_loader))\n    print(f\"✅ Successfully loaded batch: {sample_batch[0].shape}, {sample_batch[1].shape}\")\n    print(f\"✅ Memory-efficient loading working correctly\")\n    \nexcept Exception as e:\n    print(f\"❌ Error creating data loaders: {e}\")\n    print(\"Check that the dataset path is correct and data exists\")\n\n# Memory check\nimport psutil\nprocess = psutil.Process(os.getpid())\nmemory_mb = process.memory_info().rss / 1024 / 1024\nprint(f\"Current memory usage: {memory_mb:.1f} MB\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Data loading and transforms are now handled by extracted modules\nprint(\"Data loading configuration:\")\nprint(f\"  Image size: {config.image_size}\")\nprint(f\"  Batch size: {config.batch_size}\")\nprint(f\"  Data augmentation: Random flips, rotations, color jittering\")\nprint(f\"  Normalization: ImageNet pretrained values\")\nprint(f\"  Memory-efficient: On-demand image loading from paths\")\n\n# Display data loader information\nprint(f\"\\nDataLoader details:\")\nprint(f\"  Training batches: {len(train_loader)}\")\nprint(f\"  Validation batches: {len(val_loader)}\")\nprint(f\"  Test batches: {len(test_loader)}\")\n\n# Verify transforms are working by checking a sample\nsample_images, sample_labels = next(iter(train_loader))\nprint(f\"\\nSample batch verification:\")\nprint(f\"  Image tensor shape: {sample_images.shape}\")\nprint(f\"  Label tensor shape: {sample_labels.shape}\")\nprint(f\"  Image value range: [{sample_images.min():.3f}, {sample_images.max():.3f}]\")\nprint(f\"  Labels: {sample_labels[:5].tolist()}\")\n\nprint(f\"\\n✅ Data pipeline ready for training with extracted modules\")\n\n# Clean up sample to free memory\ndel sample_images, sample_labels\nimport gc\ngc.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "def visualize_batch(data_loader, class_names, title=\"Sample Images\"):\n",
    "    \"\"\"Visualize a batch of images.\"\"\"\n",
    "    data_iter = iter(data_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    # Denormalize images for visualization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    images_denorm = images * std + mean\n",
    "    images_denorm = torch.clamp(images_denorm, 0, 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(min(8, len(images))):\n",
    "        img = images_denorm[i].permute(1, 2, 0)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f'{class_names[labels[i]]}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_batch(train_loader, class_names, \"Training Samples from Unified Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture Design\n",
    "\n",
    "Now let's design our custom CNN architecture. We'll create a modular design with configurable depth and features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Model architecture is now in the extracted modules\nprint(\"Using extracted DeepLearningV1 model from src.model module\")\nprint(\"Model features:\")\nprint(\"- Custom CNN architecture for image classification\")\nprint(\"- Optimized for 128x128 input images\")\nprint(\"- 5 convolutional blocks with batch normalization\")\nprint(\"- Progressive channel increase: 3 → 32 → 64 → 128 → 256 → 512\")\nprint(\"- Global average pooling for dimensionality reduction\")\nprint(\"- Dropout regularization\")\nprint(\"- Configurable number of classes\")\nprint(\"- Kaiming weight initialization\")\n\n# Display model configuration\nprint(f\"\\nModel configuration:\")\nprint(f\"  Input channels: {config.input_channels}\")\nprint(f\"  Dropout rate: {config.dropout_rate}\")\nprint(f\"  Number of classes: {len(class_names)}\")\nprint(f\"  Input size: {config.image_size}\")\n\nprint(f\"\\n✅ Model architecture ready from extracted modules\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Create model instance using extracted modules\nprint(\"Creating model using extracted DeepLearningV1 class...\")\n\n# Create model with dynamic class discovery\nnum_classes = len(class_names)\npytorch_model = DeepLearningV1(\n    num_classes=num_classes,\n    input_channels=config.input_channels,\n    dropout_rate=config.dropout_rate\n).to(device)\n\nprint(f\"Model created for {num_classes} classes with {config.image_size[0]}x{config.image_size[1]} input\")\nprint(f\"Classes: {class_names[:5]}{'...' if len(class_names) > 5 else ''}\")\n\n# Print model summary\nprint(\"\\nModel Architecture Summary:\")\ntry:\n    print(summary(pytorch_model, input_size=(config.batch_size, 3, config.image_size[0], config.image_size[1]), device=str(device)))\nexcept Exception as e:\n    print(f\"Could not display torchinfo summary: {e}\")\n    print(\"Model created successfully without detailed summary\")\n\n# Get model information from extracted module\nmodel_info = pytorch_model.get_model_info()\nprint(f\"\\nModel Information:\")\nprint(f\"  Total parameters: {model_info['total_parameters']:,}\")\nprint(f\"  Trainable parameters: {model_info['trainable_parameters']:,}\")\nprint(f\"  Model size: {model_info['model_size_mb']:.1f} MB\")\nprint(f\"  Architecture: {model_info['architecture']}\")\n\n# Test forward pass with sample data\nprint(f\"\\nTesting forward pass...\")\ntry:\n    sample_batch = next(iter(train_loader))\n    with torch.no_grad():\n        sample_input = sample_batch[0].to(device)\n        sample_output = pytorch_model(sample_input)\n        print(f\"✅ Forward pass successful: {sample_input.shape} -> {sample_output.shape}\")\nexcept Exception as e:\n    print(f\"❌ Forward pass failed: {e}\")\n\nprint(f\"\\n✅ Model ready for training using extracted modules\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Training management is now handled by the extracted modules\nprint(\"Using extracted DeepLearningV1Trainer from src.trainer module\")\nprint(\"Trainer features:\")\nprint(\"- Early stopping with configurable patience\")\nprint(\"- Learning rate scheduling\")\nprint(\"- Training history tracking\")\nprint(\"- Best model state preservation\")\nprint(\"- Comprehensive logging and metrics\")\nprint(\"- Overfitting detection and analysis\")\nprint(\"- Memory-efficient batch processing\")\nprint(\"- GPU/CPU compatibility\")\n\n# Display trainer configuration\nprint(f\"\\nTrainer configuration:\")\nprint(f\"  Early stopping patience: {config.patience} epochs\")\nprint(f\"  Minimum improvement delta: {config.min_delta}\")\nprint(f\"  Learning rate: {config.learning_rate}\")\nprint(f\"  Weight decay: {config.weight_decay}\")\nprint(f\"  LR scheduler step size: {config.lr_step_size}\")\nprint(f\"  LR scheduler gamma: {config.lr_gamma}\")\n\nprint(f\"\\n✅ Training management ready from extracted modules\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Create and use the extracted trainer\nprint(\"Setting up training using extracted modules...\")\n\n# Create classifier and trainer\nclassifier = DeepLearningV1Classifier(\n    model_name=\"deep-learning-v1\",\n    version=\"1.0.0\",\n    config=config,\n    class_names=class_names\n)\n\n# Set the PyTorch model\nclassifier.model = pytorch_model\nclassifier.num_classes = num_classes\n\n# Create trainer\ntrainer = DeepLearningV1Trainer(classifier, config)\n\nprint(f\"Trainer created for {num_classes} classes\")\nprint(f\"Using device: {device}\")\n\n# Start training using the extracted trainer\nprint(f\"\\n🚀 Starting training with extracted trainer...\")\nprint(f\"   - Early stopping patience: {config.patience} epochs\")\nprint(f\"   - Learning rate: {config.learning_rate}\")\nprint(f\"   - Batch size: {config.batch_size}\")\nprint(f\"   - Maximum epochs: {config.num_epochs}\")\n\ntry:\n    # The trainer will handle the complete training pipeline\n    results = trainer.train(dataset_path)\n    \n    print(f\"\\n✅ Training completed using extracted modules!\")\n    print(f\"   - Test accuracy: {results['metrics']['test_accuracy']:.2f}%\")\n    print(f\"   - Best validation accuracy: {results['metrics']['best_val_accuracy']:.2f}%\")\n    print(f\"   - Training time: {results['metrics']['training_time']:.2f} seconds\")\n    print(f\"   - Epochs trained: {results['metrics']['epochs_trained']}\")\n    \n    # Store results for later use\n    trained_model = results['model']\n    training_metrics = results['metrics']\n    training_history = results['training_history']\n    \nexcept Exception as e:\n    print(f\"❌ Training failed: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Model evaluation is handled by the extracted trainer\nprint(\"Model evaluation completed during training using extracted modules\")\n\nif 'training_metrics' in locals():\n    print(f\"\\nFinal Evaluation Results:\")\n    print(f\"  Test Accuracy: {training_metrics['test_accuracy']:.2f}%\")\n    print(f\"  Best Validation Accuracy: {training_metrics['best_val_accuracy']:.2f}%\")\n    print(f\"  Training Samples: {training_metrics['train_samples']:,}\")\n    print(f\"  Validation Samples: {training_metrics['val_samples']:,}\")\n    print(f\"  Test Samples: {training_metrics['test_samples']:,}\")\n    print(f\"  Number of Classes: {training_metrics['num_classes']}\")\n    print(f\"  Training Time: {training_metrics['training_time']:.2f} seconds\")\n    \n    # Plot training history if available\n    if 'training_history' in locals():\n        print(f\"\\nPlotting training progress...\")\n        \n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n        \n        epochs = range(1, len(training_history['train_losses']) + 1)\n        \n        # Loss plot\n        ax1.plot(epochs, training_history['train_losses'], 'b-', label='Training Loss', linewidth=2)\n        ax1.plot(epochs, training_history['val_losses'], 'r-', label='Validation Loss', linewidth=2)\n        ax1.set_title('Training and Validation Loss')\n        ax1.set_xlabel('Epoch')\n        ax1.set_ylabel('Loss')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        \n        # Accuracy plot\n        ax2.plot(epochs, training_history['train_accuracies'], 'b-', label='Training Accuracy', linewidth=2)\n        ax2.plot(epochs, training_history['val_accuracies'], 'r-', label='Validation Accuracy', linewidth=2)\n        ax2.set_title('Training and Validation Accuracy')\n        ax2.set_xlabel('Epoch')\n        ax2.set_ylabel('Accuracy (%)')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        print(f\"✅ Training progress visualization complete\")\nelse:\n    print(\"❌ Training results not available - run training first\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some test predictions\n",
    "def visualize_predictions(model, test_loader, class_names, device, num_samples=8):\n",
    "    \"\"\"Visualize model predictions on test samples.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of test data\n",
    "    data_iter = iter(test_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        images_gpu = images.to(device)\n",
    "        outputs = model(images_gpu)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Denormalize images for visualization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    images_denorm = images * std + mean\n",
    "    images_denorm = torch.clamp(images_denorm, 0, 1)\n",
    "    \n",
    "    # Plot predictions\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(min(num_samples, len(images))):\n",
    "        img = images_denorm[i].permute(1, 2, 0)\n",
    "        true_label = class_names[labels[i]]\n",
    "        pred_label = class_names[predicted[i]]\n",
    "        confidence = probabilities[i][predicted[i]].item()\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        \n",
    "        # Color based on correctness\n",
    "        color = 'green' if labels[i] == predicted[i] else 'red'\n",
    "        \n",
    "        axes[i].set_title(\n",
    "            f'True: {true_label}\\nPred: {pred_label}\\nConf: {confidence:.2f}',\n",
    "            color=color\n",
    "        )\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Test Predictions (Green=Correct, Red=Incorrect)', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(model, test_loader, full_dataset.class_names, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance by class\n",
    "def analyze_per_class_performance(targets, predictions, class_names):\n",
    "    \"\"\"Analyze performance for each class.\"\"\"\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    \n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        targets, predictions, average=None, labels=range(len(class_names))\n",
    "    )\n",
    "    \n",
    "    # Create DataFrame for easy visualization\n",
    "    import pandas as pd\n",
    "    \n",
    "    performance_df = pd.DataFrame({\n",
    "        'Class': class_names,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Support': support\n",
    "    })\n",
    "    \n",
    "    print(\"Per-Class Performance:\")\n",
    "    print(performance_df.round(3))\n",
    "    \n",
    "    # Plot metrics\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "    for i, metric in enumerate(metrics):\n",
    "        axes[i].bar(class_names, performance_df[metric])\n",
    "        axes[i].set_title(f'{metric} by Class')\n",
    "        axes[i].set_ylabel(metric)\n",
    "        axes[i].set_ylim(0, 1.1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for j, v in enumerate(performance_df[metric]):\n",
    "            axes[i].text(j, v + 0.02, f'{v:.3f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return performance_df\n",
    "\n",
    "performance_df = analyze_per_class_performance(targets, predictions, full_dataset.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature visualization - show what the model learned\n",
    "def visualize_conv_filters(model, layer_name='conv1'):\n",
    "    \"\"\"Visualize convolutional filters.\"\"\"\n",
    "    # Get the layer\n",
    "    layer = getattr(model, layer_name)\n",
    "    filters = layer.weight.data.cpu()\n",
    "    \n",
    "    # Normalize filters for visualization\n",
    "    filters = (filters - filters.min()) / (filters.max() - filters.min())\n",
    "    \n",
    "    # Plot first 16 filters\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(min(16, filters.shape[0])):\n",
    "        # Convert filter to displayable format\n",
    "        filter_img = filters[i].permute(1, 2, 0)\n",
    "        \n",
    "        if filter_img.shape[2] == 3:  # RGB filter\n",
    "            axes[i].imshow(filter_img)\n",
    "        else:  # Single channel\n",
    "            axes[i].imshow(filter_img[:, :, 0], cmap='gray')\n",
    "        \n",
    "        axes[i].set_title(f'Filter {i+1}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Learned Filters in {layer_name}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize first layer filters\n",
    "visualize_conv_filters(model, 'conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Integration with Core Framework"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Model integration is handled by the extracted modules\nprint(\"Using extracted DeepLearningV1Classifier from src.classifier module\")\nprint(\"Classifier features:\")\nprint(\"- BaseImageClassifier interface compliance\")\nprint(\"- Dynamic class discovery and configuration\")\nprint(\"- Preprocessing pipeline with configurable transforms\")\nprint(\"- Batch prediction support\")\nprint(\"- Feature map extraction capabilities\")\nprint(\"- Model serialization and deserialization\")\nprint(\"- GPU/CPU compatibility\")\nprint(\"- Comprehensive metadata reporting\")\n\n# Show classifier configuration\nif 'classifier' in locals():\n    print(f\"\\nClassifier Configuration:\")\n    print(f\"  Model name: {classifier.model_name}\")\n    print(f\"  Version: {classifier.version}\")\n    print(f\"  Number of classes: {classifier.num_classes}\")\n    print(f\"  Input size: {classifier.config.image_size}\")\n    print(f\"  Device: {classifier.device}\")\n    print(f\"  Model loaded: {classifier.is_loaded}\")\n    \n    # Get metadata from extracted classifier\n    try:\n        metadata = classifier.get_metadata()\n        print(f\"\\nClassifier Metadata:\")\n        print(f\"  Architecture: {metadata.get('architecture', 'N/A')}\")\n        print(f\"  Parameters: {metadata.get('parameters', 'N/A'):,}\")\n        print(f\"  Model size: {metadata.get('model_size_mb', 'N/A'):.1f} MB\")\n        print(f\"  Model type: {metadata.get('model_type', 'N/A')}\")\n    except Exception as e:\n        print(f\"Could not retrieve metadata: {e}\")\n\nprint(f\"\\n✅ Classifier integration complete using extracted modules\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Save and test the model using extracted modules\nif 'trained_model' in locals() and 'training_metrics' in locals():\n    print(\"Saving trained model using extracted classifier...\")\n    \n    # Save the model using the extracted classifier\n    model_path = \"../models/deep_v1_classifier.pth\"\n    os.makedirs(\"../models\", exist_ok=True)\n    \n    classifier.save_model(\n        model_path,\n        model=trained_model,\n        class_names=class_names,\n        accuracy=training_metrics['test_accuracy'],\n        training_history=training_history\n    )\n    \n    print(f\"Model saved to {model_path}\")\n    \n    # Test the saved model by loading it fresh\n    print(\"\\nTesting model loading and prediction...\")\n    test_classifier = DeepLearningV1Classifier()\n    test_classifier.load_model(model_path)\n    \n    # Test prediction on a sample image\n    try:\n        sample_batch = next(iter(test_loader))\n        sample_image_tensor = sample_batch[0][0]  # Get first image from batch\n        sample_label = sample_batch[1][0].item()  # Get corresponding label\n        \n        # Convert tensor back to numpy for prediction testing\n        # Denormalize the image\n        mean = torch.tensor(config.normalize_mean).view(3, 1, 1)\n        std = torch.tensor(config.normalize_std).view(3, 1, 1)\n        sample_image_denorm = sample_image_tensor * std + mean\n        sample_image_denorm = torch.clamp(sample_image_denorm, 0, 1)\n        sample_image_np = (sample_image_denorm.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n        \n        # Make prediction using extracted classifier\n        predictions = test_classifier.predict(sample_image_np)\n        \n        print(f\"\\nSample prediction test:\")\n        # Show top 5 predictions\n        sorted_preds = sorted(predictions.items(), key=lambda x: x[1], reverse=True)\n        for i, (class_name, prob) in enumerate(sorted_preds[:5]):\n            print(f\"  {i+1}. {class_name}: {prob:.4f}\")\n        \n        print(f\"\\nActual class: {class_names[sample_label]}\")\n        predicted_class = max(predictions.items(), key=lambda x: x[1])[0]\n        correct = predicted_class == class_names[sample_label]\n        print(f\"Predicted class: {predicted_class}\")\n        print(f\"Prediction correct: {'✅' if correct else '❌'}\")\n        \n    except Exception as e:\n        print(f\"Error in prediction test: {e}\")\n    \n    # Register model in registry\n    print(f\"\\nRegistering model in registry...\")\n    registry = ModelRegistry()\n    metadata = ModelMetadata(\n        name=\"deep-learning-v1\",\n        version=\"1.0.0\",\n        model_type=\"deep_v1\",\n        accuracy=training_metrics['test_accuracy'] / 100.0,  # Convert to decimal\n        training_date=\"2024-01-01\",\n        model_path=model_path,\n        config={\n            \"architecture\": \"Custom CNN\",\n            \"num_classes\": len(class_names),\n            \"input_size\": f\"{config.image_size[0]}x{config.image_size[1]}x{config.input_channels}\",\n            \"epochs_trained\": training_metrics['epochs_trained'],\n            \"optimizer\": \"Adam\",\n            \"learning_rate\": config.learning_rate,\n            \"batch_size\": config.batch_size\n        },\n        performance_metrics={\n            \"test_accuracy\": training_metrics['test_accuracy'] / 100.0,\n            \"best_val_accuracy\": training_metrics['best_val_accuracy'] / 100.0,\n            \"training_time\": training_metrics['training_time'],\n            \"training_samples\": training_metrics['train_samples'],\n            \"test_samples\": training_metrics['test_samples']\n        }\n    )\n    \n    registry.register_model(metadata)\n    print(f\"✅ Model registered with test accuracy: {training_metrics['test_accuracy']:.2f}%\")\n    print(f\"✅ Training completed successfully using extracted modules!\")\n    print(f\"   Total classes: {len(class_names)}\")\n    print(f\"   Model path: {model_path}\")\n    \nelse:\n    print(\"❌ No trained model available - run training first\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with shallow learning if available\n",
    "def compare_with_shallow_learning():\n",
    "    \"\"\"Compare performance with shallow learning baseline.\"\"\"\n",
    "    try:\n",
    "        # Try to load shallow learning results for comparison\n",
    "        shallow_registry = registry.get_model(\"shallow-classifier\")\n",
    "        \n",
    "        if shallow_registry:\n",
    "            shallow_accuracy = shallow_registry.accuracy * 100\n",
    "            deep_accuracy = test_accuracy\n",
    "            \n",
    "            print(f\"\\nModel Comparison:\")\n",
    "            print(f\"Shallow Learning Accuracy: {shallow_accuracy:.2f}%\")\n",
    "            print(f\"Deep Learning v1 Accuracy: {deep_accuracy:.2f}%\")\n",
    "            print(f\"Improvement: {deep_accuracy - shallow_accuracy:.2f}%\")\n",
    "            \n",
    "            # Plot comparison\n",
    "            models = ['Shallow Learning', 'Deep Learning v1']\n",
    "            accuracies = [shallow_accuracy, deep_accuracy]\n",
    "            \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            bars = plt.bar(models, accuracies, color=['skyblue', 'lightcoral'])\n",
    "            plt.title('Model Performance Comparison')\n",
    "            plt.ylabel('Accuracy (%)')\n",
    "            plt.ylim(0, 100)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, acc in zip(bars, accuracies):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                        f'{acc:.1f}%', ha='center', va='bottom')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Shallow learning model not found for comparison.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not compare with shallow learning: {e}\")\n",
    "\n",
    "compare_with_shallow_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Insights\n",
    "\n",
    "### Model Architecture:\n",
    "- **Custom CNN**: 5 convolutional layers with batch normalization (optimized for 128x128 input)\n",
    "- **Feature Progression**: 3 → 32 → 64 → 128 → 256 → 512 channels\n",
    "- **Input Resolution**: 128x128x3 (sweet spot between detail and model capacity)\n",
    "- **Regularization**: Dropout, batch normalization, data augmentation\n",
    "- **Global Average Pooling**: Reduces overfitting compared to fully connected layers\n",
    "\n",
    "### Training Strategy:\n",
    "- **Optimal Resolution**: 128x128 balances detail capture with computational efficiency\n",
    "- **Memory-Efficient Loading**: On-demand image loading to handle large datasets\n",
    "- **Data Augmentation**: Random flips, rotations, color jittering\n",
    "- **Optimization**: Adam optimizer with learning rate scheduling\n",
    "- **Early Stopping**: Based on validation accuracy\n",
    "- **Monitoring**: Real-time loss and accuracy tracking\n",
    "\n",
    "### Resolution Analysis Results:\n",
    "1. **64x64**: Best performance initially due to simpler learning task\n",
    "2. **256x256**: Too much detail for current model capacity, may overfit\n",
    "3. **128x128**: Expected sweet spot - enough detail without overwhelming the model\n",
    "\n",
    "### Key Benefits of 128x128:\n",
    "- **Balanced Complexity**: 4x more detail than 64x64, but manageable for training\n",
    "- **Better Memory Usage**: More efficient than 256x256, allows larger batch sizes\n",
    "- **Optimal Learning**: Sufficient detail for discrimination without overfitting\n",
    "- **Faster Training**: Quicker than 256x256 while maintaining good accuracy\n",
    "\n",
    "### Memory Considerations:\n",
    "- **Image Size**: 128x128 = 4x more pixels than 64x64\n",
    "- **Batch Size**: Can maintain batch_size=8 with good GPU memory usage\n",
    "- **Model Complexity**: 5 layers handle 128x128 efficiently\n",
    "- **Training Speed**: Good balance between speed and accuracy\n",
    "\n",
    "### Expected Performance:\n",
    "- **Better than 64x64**: More detail for fine-grained classification\n",
    "- **Better than 256x256**: Avoids overfitting and excessive computational load\n",
    "- **Optimal Training**: Model capacity matches input complexity\n",
    "- **Good Convergence**: Expected stable training with good final accuracy\n",
    "\n",
    "### Production Readiness:\n",
    "- Model integrated with core framework\n",
    "- Saved in portable format for deployment\n",
    "- Compatible with ensemble classifier\n",
    "- Ready for API integration with 128x128 input standardization\n",
    "\n",
    "### Next Steps:\n",
    "1. **Performance Validation**: Confirm 128x128 outperforms both 64x64 and 256x256\n",
    "2. **Hyperparameter Tuning**: Optimize learning rate and batch size for 128x128\n",
    "3. **Architecture Refinement**: Consider adding skip connections if needed\n",
    "4. **Ensemble Integration**: Combine with other models for maximum accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}