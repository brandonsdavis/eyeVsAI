{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory-Efficient Shallow Learning for Full Dataset\n",
    "\n",
    "This notebook implements a streaming approach to handle the full 12,870 image dataset without memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 12:18:34.629251: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-27 12:18:34.648507: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete - streaming approach ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 12:18:35.426439: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import tempfile\n",
    "import joblib\n",
    "from typing import Generator, List, Tuple\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('../..')\n",
    "from ml_models_core.src.base_classifier import BaseImageClassifier\n",
    "from ml_models_core.src.model_registry import ModelRegistry, ModelMetadata\n",
    "from ml_models_core.src.utils import ModelUtils\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"Setup complete - streaming approach ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Dataset not found in any of the expected locations",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dataset_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataset not found in any of the expected locations\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Quick scan to get basic info\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Dataset not found in any of the expected locations"
     ]
    }
   ],
   "source": [
    "# First, let's just get the dataset structure without loading anything\n",
    "dataset_paths = [\n",
    "    \"/home/brandond/Projects/pvt/personal/image_game/data/downloads/combined_unified_classification\",\n",
    "    \"/home/brandond/Projects/pvt/personal/image_game/image-classifier-shallow/notebooks/data/downloads/combined_unified_classification\"\n",
    "]\n",
    "\n",
    "dataset_path = None\n",
    "for path in dataset_paths:\n",
    "    if os.path.exists(path):\n",
    "        dataset_path = path\n",
    "        break\n",
    "\n",
    "if dataset_path is None:\n",
    "    raise FileNotFoundError(\"Dataset not found in any of the expected locations\")\n",
    "\n",
    "print(f\"Using dataset: {dataset_path}\")\n",
    "\n",
    "# Quick scan to get basic info\n",
    "class_dirs = [d for d in Path(dataset_path).iterdir() if d.is_dir() and not d.name.startswith('.')]\n",
    "class_names = sorted([d.name for d in class_dirs])\n",
    "print(f\"Found {len(class_names)} classes\")\n",
    "print(f\"First 10 classes: {class_names[:10]}\")\n",
    "print(f\"Last 10 classes: {class_names[-10:]}\")\n",
    "\n",
    "class StreamingDatasetProcessor:\n",
    "    \"\"\"Process large datasets without loading everything into memory.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path: str, image_size: Tuple[int, int] = (64, 64)):\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.image_size = image_size\n",
    "        self.class_names = None\n",
    "        self.class_to_idx = None\n",
    "        self._scan_classes()\n",
    "    \n",
    "    def _scan_classes(self):\n",
    "        \"\"\"Scan for class directories only.\"\"\"\n",
    "        class_dirs = [d for d in self.dataset_path.iterdir() \n",
    "                     if d.is_dir() and not d.name.startswith('.')]\n",
    "        self.class_names = sorted([d.name for d in class_dirs])\n",
    "        self.class_to_idx = {name: idx for idx, name in enumerate(self.class_names)}\n",
    "        print(f\"Found {len(self.class_names)} classes\")\n",
    "    \n",
    "    def get_file_paths_generator(self) -> Generator[Tuple[str, int], None, None]:\n",
    "        \"\"\"Generator that yields (image_path, label) without loading images.\"\"\"\n",
    "        valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
    "        \n",
    "        for class_dir in self.dataset_path.iterdir():\n",
    "            if not class_dir.is_dir() or class_dir.name.startswith('.'):\n",
    "                continue\n",
    "                \n",
    "            class_idx = self.class_to_idx[class_dir.name]\n",
    "            \n",
    "            for img_path in class_dir.iterdir():\n",
    "                if img_path.suffix.lower() in valid_extensions:\n",
    "                    yield str(img_path), class_idx\n",
    "    \n",
    "    def count_total_images(self) -> int:\n",
    "        \"\"\"Count total images without loading them.\"\"\"\n",
    "        count = 0\n",
    "        for _ in self.get_file_paths_generator():\n",
    "            count += 1\n",
    "        return count\n",
    "    \n",
    "    def load_image_batch(self, paths_and_labels: List[Tuple[str, int]]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Load a batch of images.\"\"\"\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        for path, label in paths_and_labels:\n",
    "            try:\n",
    "                img = Image.open(path).convert('RGB')\n",
    "                img = img.resize(self.image_size, Image.Resampling.LANCZOS)\n",
    "                img_array = np.array(img, dtype=np.uint8)\n",
    "                images.append(img_array)\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {path}: {e}\")\n",
    "                # Skip corrupted images\n",
    "                continue\n",
    "        \n",
    "        return np.array(images), np.array(labels)\n",
    "    \n",
    "    def get_batch_generator(self, batch_size: int = 100) -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "        \"\"\"Generator that yields batches of (images, labels).\"\"\"\n",
    "        batch_paths_labels = []\n",
    "        \n",
    "        for path, label in self.get_file_paths_generator():\n",
    "            batch_paths_labels.append((path, label))\n",
    "            \n",
    "            if len(batch_paths_labels) >= batch_size:\n",
    "                images, labels = self.load_image_batch(batch_paths_labels)\n",
    "                if len(images) > 0:  # Only yield if we have valid images\n",
    "                    yield images, labels\n",
    "                batch_paths_labels = []\n",
    "                gc.collect()  # Clean up after each batch\n",
    "        \n",
    "        # Handle remaining images\n",
    "        if batch_paths_labels:\n",
    "            images, labels = self.load_image_batch(batch_paths_labels)\n",
    "            if len(images) > 0:\n",
    "                yield images, labels\n",
    "\n",
    "# Initialize the streaming processor\n",
    "processor = StreamingDatasetProcessor(dataset_path)\n",
    "print(f\"Classes: {processor.class_names}\")\n",
    "\n",
    "# Count total images\n",
    "print(\"Counting total images...\")\n",
    "total_images = processor.count_total_images()\n",
    "print(f\"Total images: {total_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingFeatureExtractor:\n",
    "    \"\"\"Extract features from images in streaming fashion.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = None  # Will initialize after seeing feature dimensions\n",
    "        self.is_fitted = False\n",
    "        self.feature_dim = None\n",
    "    \n",
    "    def extract_features_batch(self, images: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Extract features from a batch of images.\"\"\"\n",
    "        batch_features = []\n",
    "        \n",
    "        for img in images:\n",
    "            features = []\n",
    "            \n",
    "            # Convert to grayscale\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "            \n",
    "            # Basic statistics for each RGB channel (6 features x 3 channels = 18)\n",
    "            for channel in range(3):\n",
    "                channel_data = img[:, :, channel].flatten()\n",
    "                features.extend([\n",
    "                    np.mean(channel_data),\n",
    "                    np.std(channel_data),\n",
    "                    np.min(channel_data),\n",
    "                    np.max(channel_data),\n",
    "                    np.percentile(channel_data, 25),\n",
    "                    np.percentile(channel_data, 75)\n",
    "                ])\n",
    "            \n",
    "            # Grayscale statistics (3 features)\n",
    "            gray_flat = gray.flatten()\n",
    "            features.extend([\n",
    "                np.mean(gray_flat),\n",
    "                np.std(gray_flat),\n",
    "                np.var(gray_flat)\n",
    "            ])\n",
    "            \n",
    "            # Color histogram features (8 bins x 3 channels = 24)\n",
    "            for channel in range(3):\n",
    "                hist, _ = np.histogram(img[:, :, channel], bins=8, range=(0, 256))\n",
    "                hist = hist / (np.sum(hist) + 1e-8)  # Normalize with small epsilon\n",
    "                features.extend(hist)\n",
    "            \n",
    "            # Edge and texture features (5 features)\n",
    "            edges = cv2.Canny(gray, 50, 150)\n",
    "            features.extend([\n",
    "                np.sum(edges > 0) / edges.size,  # Edge density\n",
    "                np.mean(edges),\n",
    "                np.std(edges)\n",
    "            ])\n",
    "            \n",
    "            # Simple texture measures (2 features)\n",
    "            grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "            grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "            gradient_mag = np.sqrt(grad_x**2 + grad_y**2)\n",
    "            \n",
    "            features.extend([\n",
    "                np.mean(gradient_mag),\n",
    "                np.std(gradient_mag)\n",
    "            ])\n",
    "            \n",
    "            batch_features.append(features)\n",
    "        \n",
    "        result = np.array(batch_features)\n",
    "        return result\n",
    "    \n",
    "    def fit_transform_streaming(self, processor: StreamingDatasetProcessor, \n",
    "                               batch_size: int = 100) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "        \"\"\"Fit scaler and PCA incrementally and transform data.\"\"\"\n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        batch_count = 0\n",
    "        \n",
    "        print(\"Phase 1: Fitting scaler and PCA incrementally...\")\n",
    "        \n",
    "        # First pass: fit scaler and PCA\n",
    "        for images, labels in processor.get_batch_generator(batch_size):\n",
    "            batch_count += 1\n",
    "            print(f\"Processing batch {batch_count} ({len(images)} images)\")\n",
    "            \n",
    "            # Extract features\n",
    "            features = self.extract_features_batch(images)\n",
    "            print(f\"Extracted features shape: {features.shape}\")\n",
    "            \n",
    "            # Initialize PCA on first batch based on actual feature dimensions\n",
    "            if self.pca is None:\n",
    "                self.feature_dim = features.shape[1]\n",
    "                # Conservative: use at most min(20, feature_dim-1, batch_size-1)\n",
    "                n_components = min(20, self.feature_dim - 1, len(features) - 1)\n",
    "                n_components = max(1, n_components)  # Ensure at least 1 component\n",
    "                \n",
    "                print(f\"Feature dimensions: {self.feature_dim}\")\n",
    "                print(f\"Batch size: {len(features)}\")\n",
    "                print(f\"Initializing PCA with {n_components} components\")\n",
    "                \n",
    "                if n_components < self.feature_dim and len(features) > n_components:\n",
    "                    self.pca = IncrementalPCA(n_components=n_components, batch_size=min(50, len(features)))\n",
    "                    print(f\"PCA initialized successfully\")\n",
    "                else:\n",
    "                    print(f\"Skipping PCA - using raw scaled features\")\n",
    "                    self.pca = None\n",
    "            \n",
    "            # Fit scaler incrementally\n",
    "            self.scaler.partial_fit(features)\n",
    "            \n",
    "            # Transform features\n",
    "            features_scaled = self.scaler.transform(features)\n",
    "            \n",
    "            # Apply PCA if available and conditions are met\n",
    "            if (self.pca is not None and \n",
    "                len(features_scaled) > self.pca.n_components and \n",
    "                features_scaled.shape[1] > self.pca.n_components):\n",
    "                \n",
    "                self.pca.partial_fit(features_scaled)\n",
    "                features_final = self.pca.transform(features_scaled)\n",
    "                print(f\"Applied PCA: {features_scaled.shape} -> {features_final.shape}\")\n",
    "            else:\n",
    "                features_final = features_scaled\n",
    "                print(f\"Using scaled features without PCA: {features_final.shape}\")\n",
    "            \n",
    "            all_features.append(features_final)\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del images, features, features_scaled, features_final\n",
    "            gc.collect()\n",
    "            \n",
    "            # Break after a few batches for testing\n",
    "            if batch_count >= 5:\n",
    "                print(f\"Stopping after {batch_count} batches for testing...\")\n",
    "                break\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        print(f\"Fitted on {batch_count} batches\")\n",
    "        if self.pca is not None and hasattr(self.pca, 'explained_variance_ratio_'):\n",
    "            print(f\"PCA explained variance ratio: {self.pca.explained_variance_ratio_.sum():.3f}\")\n",
    "        else:\n",
    "            print(\"No PCA applied - using scaled features\")\n",
    "        \n",
    "        return all_features, all_labels\n",
    "    \n",
    "    def transform_batch(self, images: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Transform a batch of images using fitted transformers.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Must fit the transformer first\")\n",
    "        \n",
    "        features = self.extract_features_batch(images)\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        \n",
    "        if (self.pca is not None and \n",
    "            hasattr(self.pca, 'components_') and \n",
    "            features_scaled.shape[1] == self.pca.n_features_in_):\n",
    "            features_pca = self.pca.transform(features_scaled)\n",
    "            return features_pca\n",
    "        else:\n",
    "            return features_scaled\n",
    "\n",
    "# Test feature extraction first\n",
    "print(\"Testing feature extraction...\")\n",
    "test_processor = StreamingDatasetProcessor(dataset_path)\n",
    "\n",
    "# Get one small batch to test\n",
    "test_batch = None\n",
    "for images, labels in test_processor.get_batch_generator(batch_size=5):\n",
    "    test_batch = (images, labels)\n",
    "    break\n",
    "\n",
    "if test_batch:\n",
    "    test_images, test_labels = test_batch\n",
    "    print(f\"Test batch: {test_images.shape}, labels: {test_labels.shape}\")\n",
    "    \n",
    "    # Test feature extraction\n",
    "    test_extractor = StreamingFeatureExtractor()\n",
    "    test_features = test_extractor.extract_features_batch(test_images)\n",
    "    print(f\"Test features shape: {test_features.shape}\")\n",
    "    print(f\"Feature count breakdown:\")\n",
    "    print(f\"- RGB stats: 18 (6 per channel)\")\n",
    "    print(f\"- Grayscale stats: 3\")\n",
    "    print(f\"- Histograms: 24 (8 bins x 3 channels)\")\n",
    "    print(f\"- Edge features: 3\")\n",
    "    print(f\"- Texture features: 2\")\n",
    "    print(f\"- Expected total: 50\")\n",
    "    print(f\"- Actual total: {test_features.shape[1]}\")\n",
    "    \n",
    "    # Clean up test data\n",
    "    del test_images, test_labels, test_features, test_extractor, test_batch\n",
    "    gc.collect()\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = StreamingFeatureExtractor()\n",
    "print(\"Feature extractor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from full dataset using streaming approach\n",
    "print(\"Starting streaming feature extraction on full dataset...\")\n",
    "all_features, all_labels = feature_extractor.fit_transform_streaming(processor, batch_size=50)\n",
    "\n",
    "# Combine all features and labels\n",
    "print(\"Combining extracted features...\")\n",
    "X = np.vstack(all_features)\n",
    "y = np.concatenate(all_labels)\n",
    "\n",
    "print(f\"Final feature matrix shape: {X.shape}\")\n",
    "print(f\"Final labels shape: {y.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "\n",
    "# Clean up intermediate arrays\n",
    "del all_features, all_labels\n",
    "gc.collect()\n",
    "\n",
    "# Check memory usage\n",
    "import psutil\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "print(f\"Current memory usage: {memory_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training\n",
    "print(\"Splitting data for training...\")\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Clean up full dataset from memory\n",
    "del X, y, X_temp, y_temp\n",
    "gc.collect()\n",
    "\n",
    "# Use memory-efficient models\n",
    "print(\"\\nTraining memory-efficient models...\")\n",
    "\n",
    "# SGD Classifier (memory efficient)\n",
    "sgd_model = SGDClassifier(random_state=42, max_iter=1000)\n",
    "sgd_model.fit(X_train, y_train)\n",
    "sgd_val_acc = accuracy_score(y_val, sgd_model.predict(X_val))\n",
    "print(f\"SGD Validation Accuracy: {sgd_val_acc:.4f}\")\n",
    "\n",
    "# Random Forest (limited trees for memory)\n",
    "rf_model = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_val_acc = accuracy_score(y_val, rf_model.predict(X_val))\n",
    "print(f\"Random Forest Validation Accuracy: {rf_val_acc:.4f}\")\n",
    "\n",
    "# Choose best model\n",
    "if rf_val_acc > sgd_val_acc:\n",
    "    best_model = rf_model\n",
    "    best_name = \"Random Forest\"\n",
    "    best_val_acc = rf_val_acc\n",
    "else:\n",
    "    best_model = sgd_model\n",
    "    best_name = \"SGD Classifier\"\n",
    "    best_val_acc = sgd_val_acc\n",
    "\n",
    "print(f\"\\nBest model: {best_name} with validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# Test on test set\n",
    "test_predictions = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Memory check\n",
    "memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "print(f\"Final memory usage: {memory_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "class StreamingShallowClassifier(BaseImageClassifier):\n",
    "    def __init__(self, model_name=\"streaming-shallow-classifier\", version=\"1.0.0\"):\n",
    "        super().__init__(model_name, version)\n",
    "        self.model = None\n",
    "        self.feature_extractor = None\n",
    "        self.class_names = None\n",
    "    \n",
    "    def load_model(self, model_path: str) -> None:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        self.model = model_data['model']\n",
    "        self.feature_extractor = model_data['feature_extractor']\n",
    "        self.class_names = model_data['class_names']\n",
    "        self._is_loaded = True\n",
    "    \n",
    "    def preprocess(self, image: np.ndarray) -> np.ndarray:\n",
    "        image_resized = ModelUtils.resize_image(image, (64, 64))\n",
    "        if len(image_resized.shape) == 3 and image_resized.shape[2] == 4:\n",
    "            image_resized = ModelUtils.convert_to_rgb(image_resized)\n",
    "        if image_resized.max() <= 1.0:\n",
    "            image_resized = (image_resized * 255).astype(np.uint8)\n",
    "        return image_resized\n",
    "    \n",
    "    def predict(self, image: np.ndarray) -> dict:\n",
    "        if not self.is_loaded:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        processed_image = self.preprocess(image)\n",
    "        features = self.feature_extractor.transform_batch(np.array([processed_image]))\n",
    "        \n",
    "        if hasattr(self.model, 'predict_proba'):\n",
    "            probabilities = self.model.predict_proba(features)[0]\n",
    "        else:\n",
    "            # For SGD, create pseudo-probabilities\n",
    "            prediction = self.model.predict(features)[0]\n",
    "            probabilities = np.zeros(len(self.class_names))\n",
    "            probabilities[prediction] = 1.0\n",
    "        \n",
    "        return {self.class_names[i]: float(prob) for i, prob in enumerate(probabilities)}\n",
    "    \n",
    "    def get_metadata(self) -> dict:\n",
    "        return {\n",
    "            \"model_type\": \"streaming_shallow_learning\",\n",
    "            \"algorithm\": type(self.model).__name__,\n",
    "            \"feature_dimensions\": self.feature_extractor.pca.n_components_,\n",
    "            \"classes\": self.class_names,\n",
    "            \"version\": self.version\n",
    "        }\n",
    "    \n",
    "    def save_model(self, model_path: str, model, feature_extractor, class_names):\n",
    "        model_data = {\n",
    "            'model': model,\n",
    "            'feature_extractor': feature_extractor,\n",
    "            'class_names': class_names\n",
    "        }\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Save the model\n",
    "model_path = \"../models/streaming_shallow_classifier.pkl\"\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "classifier = StreamingShallowClassifier()\n",
    "classifier.save_model(model_path, best_model, feature_extractor, processor.class_names)\n",
    "\n",
    "print(f\"\\nModel training complete!\")\n",
    "print(f\"Best model: {best_name}\")\n",
    "print(f\"Validation accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"Total classes: {len(processor.class_names)}\")\n",
    "print(f\"Total images processed: {len(X_train) + len(X_val) + len(X_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
