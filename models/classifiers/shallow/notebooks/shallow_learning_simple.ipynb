{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Streaming Shallow Learning - No PCA\n",
    "\n",
    "This notebook uses a simple approach without PCA to avoid dimensionality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from typing import Generator, List, Tuple\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('../..')\n",
    "from ml_models_core.src.base_classifier import BaseImageClassifier\n",
    "from ml_models_core.src.utils import ModelUtils\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete - simple streaming approach ready\")\n",
    "\n",
    "# Find dataset\n",
    "dataset_paths = [\n",
    "    \"../../data/downloads/combined_unified_classification\",\n",
    "    \"./data/downloads/combined_unified_classification\"\n",
    "]\n",
    "\n",
    "dataset_path = None\n",
    "for path in dataset_paths:\n",
    "    if os.path.exists(path):\n",
    "        dataset_path = path\n",
    "        break\n",
    "\n",
    "if dataset_path is None:\n",
    "    raise FileNotFoundError(\"Dataset not found\")\n",
    "\n",
    "print(f\"Using dataset: {dataset_path}\")\n",
    "\n",
    "# Get class info\n",
    "class_dirs = [d for d in Path(dataset_path).iterdir() if d.is_dir() and not d.name.startswith('.')]\n",
    "class_names = sorted([d.name for d in class_dirs])\n",
    "class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "\n",
    "print(f\"Found {len(class_names)} classes\")\n",
    "print(f\"First 5 classes: {class_names[:5]}\")\n",
    "print(f\"Last 5 classes: {class_names[-5:]}\")\n",
    "\n",
    "print(\"Dataset path and class mapping defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell requires that cell 1 has been run first to define dataset_path and class_to_idx\n",
    "\n",
    "def extract_simple_features(image):\n",
    "    \"\"\"Extract simple features from a single image.\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # RGB channel statistics (mean, std per channel)\n",
    "    for channel in range(3):\n",
    "        channel_data = image[:, :, channel].flatten()\n",
    "        features.extend([\n",
    "            np.mean(channel_data),\n",
    "            np.std(channel_data)\n",
    "        ])\n",
    "    \n",
    "    # Grayscale statistics\n",
    "    gray_flat = gray.flatten()\n",
    "    features.extend([\n",
    "        np.mean(gray_flat),\n",
    "        np.std(gray_flat)\n",
    "    ])\n",
    "    \n",
    "    # Simple histogram (4 bins per channel to keep features low)\n",
    "    for channel in range(3):\n",
    "        hist, _ = np.histogram(image[:, :, channel], bins=4, range=(0, 256))\n",
    "        hist = hist / (np.sum(hist) + 1e-8)\n",
    "        features.extend(hist)\n",
    "    \n",
    "    # Edge density\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    features.append(np.sum(edges > 0) / edges.size)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Test feature extraction\n",
    "def test_features():\n",
    "    # Find first image\n",
    "    for class_dir in Path(dataset_path).iterdir():\n",
    "        if class_dir.is_dir() and not class_dir.name.startswith('.'):\n",
    "            for img_path in class_dir.iterdir():\n",
    "                if img_path.suffix.lower() in {'.jpg', '.jpeg', '.png'}:\n",
    "                    # Load and test\n",
    "                    img = Image.open(img_path).convert('RGB')\n",
    "                    img = img.resize((64, 64))\n",
    "                    img_array = np.array(img)\n",
    "                    \n",
    "                    features = extract_simple_features(img_array)\n",
    "                    print(f\"Feature vector length: {len(features)}\")\n",
    "                    print(f\"Feature breakdown:\")\n",
    "                    print(f\"  RGB stats: 6 (2 per channel)\")\n",
    "                    print(f\"  Gray stats: 2\")\n",
    "                    print(f\"  Histograms: 12 (4 bins x 3 channels)\")\n",
    "                    print(f\"  Edge density: 1\")\n",
    "                    print(f\"  Total expected: 21\")\n",
    "                    print(f\"  Actual: {len(features)}\")\n",
    "                    return\n",
    "\n",
    "test_features()\n",
    "\n",
    "def process_dataset_streaming(batch_size=50, max_images_per_class=None):\n",
    "    \"\"\"Process dataset in streaming fashion. If max_images_per_class=None, use all images.\"\"\"\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
    "    \n",
    "    batch_images = []\n",
    "    batch_labels = []\n",
    "    total_processed = 0\n",
    "    \n",
    "    if max_images_per_class is None:\n",
    "        print(\"Processing FULL dataset (no image limit per class)...\")\n",
    "    else:\n",
    "        print(f\"Processing dataset with max {max_images_per_class} images per class...\")\n",
    "    \n",
    "    for class_dir in Path(dataset_path).iterdir():\n",
    "        if not class_dir.is_dir() or class_dir.name.startswith('.'):\n",
    "            continue\n",
    "            \n",
    "        class_name = class_dir.name\n",
    "        class_idx = class_to_idx[class_name]\n",
    "        class_count = 0\n",
    "        \n",
    "        print(f\"Processing class: {class_name}\")\n",
    "        \n",
    "        for img_path in class_dir.iterdir():\n",
    "            if img_path.suffix.lower() not in valid_extensions:\n",
    "                continue\n",
    "                \n",
    "            # Only check limit if it's specified\n",
    "            if max_images_per_class is not None and class_count >= max_images_per_class:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                # Load image\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img = img.resize((64, 64))\n",
    "                img_array = np.array(img, dtype=np.uint8)\n",
    "                \n",
    "                batch_images.append(img_array)\n",
    "                batch_labels.append(class_idx)\n",
    "                class_count += 1\n",
    "                total_processed += 1\n",
    "                \n",
    "                # Process batch when full\n",
    "                if len(batch_images) >= batch_size:\n",
    "                    print(f\"  Processing batch of {len(batch_images)} images... (Total so far: {total_processed})\")\n",
    "                    \n",
    "                    # Extract features\n",
    "                    batch_features = []\n",
    "                    for img in batch_images:\n",
    "                        features = extract_simple_features(img)\n",
    "                        batch_features.append(features)\n",
    "                    \n",
    "                    all_features.extend(batch_features)\n",
    "                    all_labels.extend(batch_labels)\n",
    "                    \n",
    "                    # Clear batch\n",
    "                    batch_images = []\n",
    "                    batch_labels = []\n",
    "                    gc.collect()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"    Error loading {img_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"  Completed {class_name}: {class_count} images\")\n",
    "    \n",
    "    # Process remaining batch\n",
    "    if batch_images:\n",
    "        print(f\"Processing final batch of {len(batch_images)} images...\")\n",
    "        batch_features = []\n",
    "        for img in batch_images:\n",
    "            features = extract_simple_features(img)\n",
    "            batch_features.append(features)\n",
    "        \n",
    "        all_features.extend(batch_features)\n",
    "        all_labels.extend(batch_labels)\n",
    "    \n",
    "    print(f\"\\n=== PROCESSING COMPLETE ===\")\n",
    "    print(f\"Total processed: {total_processed} images\")\n",
    "    print(f\"Total classes: {len(np.unique(all_labels))}\")\n",
    "    return np.array(all_features), np.array(all_labels)\n",
    "\n",
    "print(\"Functions defined. Ready to process dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset - NOW USING ALL DATA!\n",
    "print(\"Starting streaming processing with FULL dataset...\")\n",
    "X, y = process_dataset_streaming(batch_size=50, max_images_per_class=None)  # None = no limit\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Unique classes: {len(np.unique(y))}\")\n",
    "\n",
    "# Memory check\n",
    "import psutil\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "print(f\"Memory usage: {memory_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features and split data\n",
    "print(\"Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Splitting data...\")\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Clean up\n",
    "del X, X_scaled, X_temp, y_temp\n",
    "gc.collect()\n",
    "\n",
    "print(\"Data scaling and splitting complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "print(\"Training models...\")\n",
    "\n",
    "# SGD Classifier\n",
    "sgd_model = SGDClassifier(random_state=42, max_iter=1000, alpha=0.01)\n",
    "sgd_model.fit(X_train, y_train)\n",
    "sgd_val_acc = accuracy_score(y_val, sgd_model.predict(X_val))\n",
    "print(f\"SGD Validation Accuracy: {sgd_val_acc:.4f}\")\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_val_acc = accuracy_score(y_val, rf_model.predict(X_val))\n",
    "print(f\"Random Forest Validation Accuracy: {rf_val_acc:.4f}\")\n",
    "\n",
    "# Choose best model\n",
    "if rf_val_acc > sgd_val_acc:\n",
    "    best_model = rf_model\n",
    "    best_name = \"Random Forest\"\n",
    "    best_val_acc = rf_val_acc\n",
    "else:\n",
    "    best_model = sgd_model\n",
    "    best_name = \"SGD Classifier\"\n",
    "    best_val_acc = sgd_val_acc\n",
    "\n",
    "print(f\"\\nBest model: {best_name} ({best_val_acc:.4f})\")\n",
    "\n",
    "# Test accuracy\n",
    "test_predictions = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Show class distribution in results\n",
    "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "print(f\"\\nTest set class distribution: {len(unique_test)} classes\")\n",
    "print(f\"Images per class (mean): {np.mean(counts_test):.1f}\")\n",
    "print(f\"Images per class (min/max): {np.min(counts_test)}/{np.max(counts_test)}\")\n",
    "\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple classifier class\n",
    "class SimpleShallowClassifier(BaseImageClassifier):\n",
    "    def __init__(self, model_name=\"simple-shallow-classifier\", version=\"1.0.0\"):\n",
    "        super().__init__(model_name, version)\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.class_names = None\n",
    "    \n",
    "    def load_model(self, model_path: str) -> None:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        self.model = model_data['model']\n",
    "        self.scaler = model_data['scaler']\n",
    "        self.class_names = model_data['class_names']\n",
    "        self._is_loaded = True\n",
    "    \n",
    "    def preprocess(self, image: np.ndarray) -> np.ndarray:\n",
    "        image_resized = ModelUtils.resize_image(image, (64, 64))\n",
    "        if len(image_resized.shape) == 3 and image_resized.shape[2] == 4:\n",
    "            image_resized = ModelUtils.convert_to_rgb(image_resized)\n",
    "        if image_resized.max() <= 1.0:\n",
    "            image_resized = (image_resized * 255).astype(np.uint8)\n",
    "        return image_resized\n",
    "    \n",
    "    def predict(self, image: np.ndarray) -> dict:\n",
    "        if not self.is_loaded:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        processed_image = self.preprocess(image)\n",
    "        features = extract_simple_features(processed_image)\n",
    "        features_scaled = self.scaler.transform([features])\n",
    "        \n",
    "        if hasattr(self.model, 'predict_proba'):\n",
    "            probabilities = self.model.predict_proba(features_scaled)[0]\n",
    "        else:\n",
    "            prediction = self.model.predict(features_scaled)[0]\n",
    "            probabilities = np.zeros(len(self.class_names))\n",
    "            probabilities[prediction] = 1.0\n",
    "        \n",
    "        return {self.class_names[i]: float(prob) for i, prob in enumerate(probabilities)}\n",
    "    \n",
    "    def get_metadata(self) -> dict:\n",
    "        return {\n",
    "            \"model_type\": \"simple_shallow_learning\",\n",
    "            \"algorithm\": type(self.model).__name__,\n",
    "            \"feature_dimensions\": 21,\n",
    "            \"classes\": self.class_names,\n",
    "            \"version\": self.version\n",
    "        }\n",
    "    \n",
    "    def save_model(self, model_path: str, model, scaler, class_names):\n",
    "        model_data = {\n",
    "            'model': model,\n",
    "            'scaler': scaler,\n",
    "            'class_names': class_names\n",
    "        }\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Save the model\n",
    "model_path = \"../models/simple_shallow_classifier.pkl\"\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "classifier = SimpleShallowClassifier()\n",
    "classifier.save_model(model_path, best_model, scaler, class_names)\n",
    "\n",
    "print(f\"\\n=== TRAINING COMPLETE ===\")\n",
    "print(f\"Model: {best_name}\")\n",
    "print(f\"Validation accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Classes: {len(class_names)}\")\n",
    "print(f\"Features: 21\")\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Final memory check\n",
    "memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "print(f\"Final memory usage: {memory_mb:.1f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
