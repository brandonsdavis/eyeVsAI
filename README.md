# Eye vs AI - Image Classification Game

A comprehensive machine learning project featuring production-ready image classification models with a full-stack web game where players compete against AI models. The system demonstrates various ML approaches from traditional feature engineering to modern deep learning architectures.

## Project Overview

This project serves as a portfolio demonstration of modern machine learning engineering practices, featuring:

### Machine Learning Pipeline
- Four distinct model architectures with different technical approaches
- Unified command-line interface for consistent model training
- Production-grade training pipeline with hyperparameter optimization
- Comprehensive model evaluation and deployment system

### Web Application
- Interactive game where users compete against AI models
- Real-time classification challenges across multiple datasets
- Performance tracking and leaderboard functionality
- Containerized deployment architecture for AWS

## ğŸ“ Project Structure

```
eyeVsAI/
â”œâ”€â”€ models/                        # ğŸ§  ML Models and Training
â”‚   â”œâ”€â”€ classifiers/               # Model implementations
â”‚   â”‚   â”œâ”€â”€ shallow/               # Traditional ML with feature extraction
â”‚   â”‚   â”‚   â”œâ”€â”€ src/               # Extracted production modules
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ config.py      # Configuration dataclass
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ feature_extractor.py  # HOG, LBP, color features
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ classifier.py  # BaseImageClassifier implementation
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ trainer.py     # Training and evaluation
â”‚   â”‚   â”‚   â”œâ”€â”€ scripts/           # CLI training scripts
â”‚   â”‚   â”‚   â””â”€â”€ notebooks/         # Jupyter notebooks for experimentation
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ deep-v1/               # Basic CNN implementation
â”‚   â”‚   â”‚   â”œâ”€â”€ src/               # PyTorch-based modules
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ config.py      # Deep learning configuration
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ model.py       # CNN architecture
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ data_loader.py # Memory-efficient data loading
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ trainer.py     # Training with GPU optimization
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ classifier.py  # Prediction interface
â”‚   â”‚   â”‚   â”œâ”€â”€ scripts/           # CLI training scripts
â”‚   â”‚   â”‚   â””â”€â”€ notebooks/         # Jupyter notebooks for experimentation
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ deep-v2/               # Advanced CNN with attention
â”‚   â”‚   â”‚   â”œâ”€â”€ src/               # Advanced PyTorch implementation
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ config.py      # Advanced training configuration
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ model.py       # ResNet + Attention mechanisms
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ data_loader.py # Lazy loading with mixup
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ trainer.py     # Memory-efficient training
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ classifier.py  # Advanced prediction interface
â”‚   â”‚   â”‚   â”œâ”€â”€ scripts/           # CLI training scripts
â”‚   â”‚   â”‚   â””â”€â”€ notebooks/         # Jupyter notebooks for experimentation
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ transfer/              # Transfer learning with pre-trained models
â”‚   â”‚       â”œâ”€â”€ src/               # TensorFlow/Keras modules
â”‚   â”‚       â”‚   â”œâ”€â”€ config.py      # Transfer learning configuration
â”‚   â”‚       â”‚   â”œâ”€â”€ models.py      # Pre-trained model integration
â”‚   â”‚       â”‚   â”œâ”€â”€ data_loader.py # TensorFlow data pipeline
â”‚   â”‚       â”‚   â”œâ”€â”€ trainer.py     # Two-phase training
â”‚   â”‚       â”‚   â””â”€â”€ classifier.py  # Transfer learning interface
â”‚   â”‚       â”œâ”€â”€ scripts/           # CLI training scripts
â”‚   â”‚       â””â”€â”€ notebooks/         # Jupyter notebooks for experimentation
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                      # Core infrastructure
â”‚   â”‚   â””â”€â”€ src/                   # Base classes and utilities
â”‚   â”‚       â”œâ”€â”€ base_classifier.py # BaseImageClassifier interface
â”‚   â”‚       â”œâ”€â”€ base_trainer.py    # BaseTrainer interface
â”‚   â”‚       â”œâ”€â”€ base_config.py     # Configuration base classes
â”‚   â”‚       â”œâ”€â”€ model_registry.py  # Model metadata management
â”‚   â”‚       â””â”€â”€ utils.py           # Utility functions
â”‚   â”‚
â”‚   â”œâ”€â”€ production/                # Production training pipeline
â”‚   â”‚   â”œâ”€â”€ configs/               # Model and dataset configurations
â”‚   â”‚   â”‚   â”œâ”€â”€ models.json        # Model type definitions
â”‚   â”‚   â”‚   â””â”€â”€ datasets.json      # Dataset configurations
â”‚   â”‚   â”œâ”€â”€ scripts/               # Training automation scripts
â”‚   â”‚   â”‚   â”œâ”€â”€ production_trainer.py        # Individual model training
â”‚   â”‚   â”‚   â”œâ”€â”€ hyperparameter_tuner.py      # Automated hyperparameter tuning
â”‚   â”‚   â”‚   â””â”€â”€ train_all_production_models.py # Master training pipeline
â”‚   â”‚   â”œâ”€â”€ models/                # Trained model outputs
â”‚   â”‚   â”œâ”€â”€ logs/                  # Training logs
â”‚   â”‚   â””â”€â”€ results/               # Training results and reports
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                      # Training datasets
â”‚   â”‚   â””â”€â”€ downloads/             # Downloaded dataset storage
â”‚   â”‚
â”‚   â””â”€â”€ checkpoints/               # Model checkpoints
â”‚
â”œâ”€â”€ backend/                       # Backend Services
â”‚   â”œâ”€â”€ api/                       # Game API server
â”‚   â”‚   â”œâ”€â”€ app/                   # FastAPI application
â”‚   â”‚   â”‚   â”œâ”€â”€ routes/            # API endpoints
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ auth.py        # Authentication & OAuth
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ game.py        # Game management
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ images.py      # Image serving
â”‚   â”‚   â”‚   â”œâ”€â”€ services/          # Business logic
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ model_manager.py      # AI model integration
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ game_backend_service.py # Game logic
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ cache_service.py      # Redis caching
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ s3_service.py         # Cloud storage
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.py            # JWT & OAuth logic
â”‚   â”‚   â”‚   â”œâ”€â”€ database.py        # Database setup
â”‚   â”‚   â”‚   â”œâ”€â”€ db_models.py       # SQLAlchemy models
â”‚   â”‚   â”‚   â””â”€â”€ main.py            # FastAPI app
â”‚   â”‚   â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚   â”‚   â”œâ”€â”€ .env.example           # Environment variables
â”‚   â”‚   â”œâ”€â”€ start.sh               # Startup script
â”‚   â”‚   â””â”€â”€ API_DOCUMENTATION.md   # API reference
â”‚   â”‚
â”‚   â””â”€â”€ deploy/                    # Deployment configuration
â”‚       â””â”€â”€ nginx.conf             # Nginx configuration
â”‚
â”œâ”€â”€ frontend/                      # Frontend Game
â”‚   â””â”€â”€ game/                      # Classification game interface
â”‚       â”œâ”€â”€ src/                   # React game implementation
â”‚       â”œâ”€â”€ Dockerfile             # Container configuration
â”‚       â””â”€â”€ package.json           # Node.js dependencies
â”‚
â”œâ”€â”€ GAME_DESIGN.md                # Game design and architecture
â”œâ”€â”€ CLAUDE.md                     # AI assistant instructions
â”‚
â”œâ”€â”€ train_models.py               # UNIFIED CLI LAUNCHER
â”œâ”€â”€ test_models.py                # Comprehensive testing suite
â”œâ”€â”€ test_structure.py             # Structural validation tests
â”œâ”€â”€ test_configs.py               # Configuration testing
â”œâ”€â”€ docker-compose.yml            # Multi-service deployment
â””â”€â”€ README.md                     # This file
```

## System Architecture

### Component Structure
The system is organized into three main components, each with dedicated documentation:

- **[models/](models/README.md)** - Machine learning pipeline with four model types
- **[frontend/](frontend/README.md)** - React-based game interface  
- **[backend/](backend/README.md)** - FastAPI server with model serving

### Design Philosophy
This project demonstrates modern ML engineering principles while maintaining focus on educational value and portfolio presentation. The architecture balances production-ready practices with single-server deployment simplicity.

## Quick Start

### Development Setup

```bash
# Clone the repository
git clone <repository-url>
cd eyeVsAI

# Set up Python environment (Python 3.11 recommended)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### Machine Learning Models

The system includes four model types with unified CLI access:

```bash
# View available models
python train_models.py list

# Train individual models
python train_models.py shallow --data_path ./data/pets
python train_models.py deep-v1 --data_path ./data/pets
python train_models.py deep-v2 --data_path ./data/pets --architecture resnet
python train_models.py transfer --data_path ./data/pets --base_model resnet50

# Run production training pipeline
cd models/production
python scripts/train_all_production_models.py \
    --models deep_v1 deep_v2 transfer \
    --parallel_jobs 2 \
    --run_tuning
```

See the [models documentation](models/README.md) for detailed usage instructions.

## Model Types

| Model | Framework | Approach | Use Case |
|-------|-----------|----------|----------|
| **Shallow Learning** | scikit-learn | Traditional ML with feature engineering | Interpretable baseline models |
| **Deep Learning v1** | PyTorch | Basic CNN architecture | Standard deep learning approach |
| **Deep Learning v2** | PyTorch | Advanced CNN with attention | State-of-the-art techniques |
| **Transfer Learning** | TensorFlow | Pre-trained model fine-tuning | Efficient training on limited data |

Each model type includes comprehensive documentation, configuration management, and production deployment capabilities. For detailed technical specifications and usage examples, see the [models documentation](models/README.md).

## AWS Deployment Guide

This system is designed for deployment on a single AWS EC2 instance using Docker containers. The following guide covers production deployment for demonstration purposes.

### Infrastructure Requirements

**Recommended EC2 Instance**: t3.xlarge or larger
- 4+ vCPUs for parallel model training (smaller instances can be used for inference-only)
- 16+ GB RAM for model serving
- 100+ GB EBS storage for models and data

**Additional AWS Services**:
- RDS PostgreSQL (db.t3.micro for demonstration)
- S3 bucket for model storage
- ElastiCache Redis (cache.t3.micro)
- Application Load Balancer (optional)

### 1. EC2 Instance Setup

```bash
# Connect to your EC2 instance
ssh -i your-key.pem ubuntu@your-instance-ip

# Update system and install Docker
sudo apt update && sudo apt upgrade -y
sudo apt install docker.io docker-compose -y
sudo systemctl enable docker
sudo usermod -aG docker ubuntu

# Clone repository
git clone <your-repo-url>
cd eyeVsAI
```

### 2. Environment Configuration

Create production environment file:

```bash
# Create .env file
cat > .env << EOF
# Database
DATABASE_URL=postgresql://username:password@your-rds-endpoint/eyevsai

# Authentication
SECRET_KEY=$(openssl rand -hex 32)
ACCESS_TOKEN_EXPIRE_MINUTES=60

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
ALLOWED_ORIGINS=https://yourdomain.com

# AWS Services
AWS_ACCESS_KEY_ID=your-access-key
AWS_SECRET_ACCESS_KEY=your-secret-key
S3_BUCKET_MODELS=your-models-bucket
S3_BUCKET_IMAGES=your-images-bucket

# Redis
REDIS_URL=redis://your-elasticache-endpoint:6379/0

# Frontend
REACT_APP_API_URL=https://yourdomain.com/api
EOF
```

### 3. Build and Deploy

```bash
# Build all containers
docker-compose -f docker-compose.prod.yml build

# Deploy services
docker-compose -f docker-compose.prod.yml up -d

# Verify deployment
docker-compose ps
curl http://localhost:8000/health
```

### 4. Model Training and Upload

```bash
# Train models for production
cd models/production
python scripts/train_all_production_models.py \
    --models deep_v1 deep_v2 transfer \
    --parallel_jobs 2 \
    --run_tuning

# Upload trained models to S3
aws s3 sync models/ s3://your-models-bucket/ --exclude "*.log"
```

### 5. Database Setup

```bash
# Initialize database schema
docker-compose exec backend python -c "
from app.database import create_tables
create_tables()
"

# Create initial admin user (optional)
docker-compose exec backend python -c "
from app.auth import create_user
create_user('admin@yourdomain.com', 'secure_password', is_admin=True)
"
```

### 6. SSL and Domain Configuration

For production deployment with a custom domain:

```bash
# Install certbot for SSL certificates
sudo apt install certbot nginx -y

# Configure nginx (update nginx.conf with your domain)
sudo cp deploy/nginx.conf /etc/nginx/sites-available/eyevsai
sudo ln -s /etc/nginx/sites-available/eyevsai /etc/nginx/sites-enabled/
sudo rm /etc/nginx/sites-enabled/default

# Obtain SSL certificate
sudo certbot --nginx -d yourdomain.com

# Start nginx
sudo systemctl enable nginx
sudo systemctl start nginx
```

### 7. Monitoring and Maintenance

```bash
# View application logs
docker-compose logs -f backend
docker-compose logs -f frontend

# Monitor system resources
htop
docker stats

# Backup database
pg_dump $DATABASE_URL > backup_$(date +%Y%m%d).sql

# Update application
git pull
docker-compose -f docker-compose.prod.yml build
docker-compose -f docker-compose.prod.yml up -d
```

### Production Considerations

**Security**:
- Configure AWS Security Groups to restrict access
- Use IAM roles instead of access keys when possible
- Enable CloudTrail for audit logging
- Set up VPC with private subnets for database

**Scalability**:
- Use Application Load Balancer for multiple instances
- Implement auto-scaling groups for traffic spikes
- Consider ECS or EKS for container orchestration
- Use CloudFront CDN for static assets

**Monitoring**:
- CloudWatch for infrastructure metrics
- Application-level logging with structured logs
- Health checks for container orchestration
- Database performance monitoring

**Cost Optimization**:
- Use Reserved Instances for predictable workloads
- Implement S3 lifecycle policies for old models
- Monitor costs with AWS Cost Explorer

### Testing Deployment

```bash
# Test API endpoints
curl https://yourdomain.com/api/health
curl https://yourdomain.com/api/models

# Test frontend
curl https://yourdomain.com/

# Load test (optional)
ab -n 100 -c 10 https://yourdomain.com/api/health
```

## Local Development

### Running Individual Components

**Backend API**:
```bash
cd backend/api
./start.sh
# Available at http://localhost:8000
```

**Frontend Game**:
```bash
cd frontend/game
npm install
npm start
# Available at http://localhost:3000
```

**Full Stack with Docker**:
```bash
docker-compose up --build
```

## Component Documentation

For detailed information about each component:

- **[Machine Learning Models](models/README.md)** - Training pipelines, model architectures, and production deployment
- **[Frontend Application](frontend/README.md)** - React game interface, components, and build process
- **[Backend API](backend/README.md)** - FastAPI server, endpoints, authentication, and model serving

## Project Features

This project demonstrates modern software engineering practices:

- **Modular Architecture**: Component-based design with clear separation of concerns
- **Production Deployment**: Docker containerization with AWS deployment guide
- **Quality Assurance**: Comprehensive testing and documentation standards
- **Scalable Design**: Architecture considerations for future growth

## License

Licensed under the Apache License, Version 2.0. See individual components for specific licensing information.
