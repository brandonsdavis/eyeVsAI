{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning v2 Image Classification Development\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Implement advanced neural network architectures with modern techniques\n",
    "- Explore residual connections, attention mechanisms, and advanced regularization\n",
    "- Improve upon Deep Learning v1 performance\n",
    "- Demonstrate state-of-the-art deep learning practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "from torchinfo import summary\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "import math\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Add parent directory to path for model core imports\n",
    "sys.path.append('../..')\n",
    "from ml_models_core.src.base_classifier import BaseImageClassifier\n",
    "from ml_models_core.src.model_registry import ModelRegistry, ModelMetadata\n",
    "from ml_models_core.src.utils import ModelUtils\n",
    "from ml_models_core.src.data_loaders import get_unified_classification_data\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient approach: Get dataset info without loading all paths at once\n",
    "print(\"Getting dataset info from data manager...\")\n",
    "\n",
    "from ml_models_core.src.data_manager import get_dataset_manager\n",
    "import gc\n",
    "\n",
    "manager = get_dataset_manager()\n",
    "\n",
    "# Get the unified dataset path\n",
    "try:\n",
    "    dataset_path = manager.get_dataset_path('combined_unified_classification')\n",
    "    if not dataset_path:\n",
    "        print(\"Creating unified classification dataset...\")\n",
    "        available_datasets = ['oxford_pets', 'kaggle_vegetables', 'street_foods', 'musical_instruments']\n",
    "        dataset_path = manager.create_combined_dataset(\n",
    "            dataset_names=available_datasets,\n",
    "            output_name=\"unified_classification\",\n",
    "            class_mapping=None  # Keep original class names\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing unified dataset: {e}\")\n",
    "    # Fallback to main dataset\n",
    "    dataset_path = manager.download_dataset('oxford_pets')\n",
    "\n",
    "print(f\"Using dataset at: {dataset_path}\")\n",
    "\n",
    "# Memory-efficient scanning: Only collect class info, not all paths\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_path = Path(dataset_path)\n",
    "class_names = []\n",
    "class_to_idx = {}\n",
    "\n",
    "# Collect all class directories\n",
    "class_dirs = [d for d in dataset_path.iterdir() \n",
    "             if d.is_dir() and not d.name.startswith('.')]\n",
    "\n",
    "if not class_dirs:\n",
    "    raise ValueError(f\"No class directories found in {dataset_path}\")\n",
    "\n",
    "class_names = sorted([d.name for d in class_dirs])\n",
    "class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "\n",
    "print(f\"Found {len(class_names)} classes\")\n",
    "\n",
    "# Count images per class WITHOUT loading all paths into memory\n",
    "valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
    "total_images = 0\n",
    "class_counts = {}\n",
    "\n",
    "print(\"Counting images per class...\")\n",
    "for class_dir in class_dirs:\n",
    "    class_name = class_dir.name\n",
    "    \n",
    "    # Count files without storing paths\n",
    "    image_count = sum(1 for f in class_dir.iterdir() \n",
    "                     if f.suffix.lower() in valid_extensions)\n",
    "    \n",
    "    class_counts[class_name] = image_count\n",
    "    total_images += image_count\n",
    "    \n",
    "    print(f\"  {class_name}: {image_count} images\")\n",
    "\n",
    "print(f\"\\nTotal images: {total_images}\")\n",
    "print(f\"Memory usage: Only storing class info (~{len(class_names) * 50} bytes) instead of all paths\")\n",
    "\n",
    "# Store dataset info for later use\n",
    "dataset_info = {\n",
    "    'dataset_path': dataset_path,\n",
    "    'class_names': class_names,\n",
    "    'class_to_idx': class_to_idx,\n",
    "    'class_counts': class_counts,\n",
    "    'total_images': total_images,\n",
    "    'valid_extensions': valid_extensions\n",
    "}\n",
    "\n",
    "# Free memory\n",
    "gc.collect()\n",
    "print(f\"✅ Memory-efficient dataset scanning completed\")\n",
    "print(f\"Classes: {len(class_names)} total\")\n",
    "print(f\"Sample classes: {class_names[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyUnifiedDataset(Dataset):\n",
    "    \"\"\"Ultra memory-efficient dataset with lazy loading and minimal memory footprint.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_info, subset_indices=None, transform=None, mixup_alpha=0.2):\n",
    "        self.dataset_path = dataset_info['dataset_path']\n",
    "        self.class_names = dataset_info['class_names']\n",
    "        self.class_to_idx = dataset_info['class_to_idx']\n",
    "        self.valid_extensions = dataset_info['valid_extensions']\n",
    "        self.transform = transform\n",
    "        self.mixup_alpha = mixup_alpha\n",
    "        \n",
    "        # Build image paths lazily only when needed\n",
    "        self._image_paths = None\n",
    "        self._labels = None\n",
    "        self.subset_indices = subset_indices\n",
    "        \n",
    "        # Calculate total length without loading paths\n",
    "        if subset_indices is not None:\n",
    "            self._length = len(subset_indices)\n",
    "        else:\n",
    "            self._length = sum(dataset_info['class_counts'].values())\n",
    "    \n",
    "    def _load_paths_lazy(self):\n",
    "        \"\"\"Load image paths only when first accessed.\"\"\"\n",
    "        if self._image_paths is None:\n",
    "            print(\"Loading image paths (first access)...\")\n",
    "            self._image_paths = []\n",
    "            self._labels = []\n",
    "            \n",
    "            for class_dir in sorted(self.dataset_path.iterdir()):\n",
    "                if not class_dir.is_dir() or class_dir.name.startswith('.'):\n",
    "                    continue\n",
    "                    \n",
    "                class_name = class_dir.name\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    continue\n",
    "                    \n",
    "                class_idx = self.class_to_idx[class_name]\n",
    "                \n",
    "                # Load paths for this class\n",
    "                image_files = [f for f in class_dir.iterdir() \n",
    "                              if f.suffix.lower() in self.valid_extensions]\n",
    "                \n",
    "                for img_path in image_files:\n",
    "                    self._image_paths.append(str(img_path))\n",
    "                    self._labels.append(class_idx)\n",
    "            \n",
    "            # Apply subset if specified\n",
    "            if self.subset_indices is not None:\n",
    "                self._image_paths = [self._image_paths[i] for i in self.subset_indices]\n",
    "                self._labels = [self._labels[i] for i in self.subset_indices]\n",
    "            \n",
    "            print(f\"Loaded {len(self._image_paths)} image paths\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Lazy load paths on first access\n",
    "        if self._image_paths is None:\n",
    "            self._load_paths_lazy()\n",
    "            \n",
    "        image_path = self._image_paths[idx]\n",
    "        label = self._labels[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load image from disk with memory-efficient handling\n",
    "            with Image.open(image_path) as img:\n",
    "                image = img.convert('RGB').copy()  # Copy to close file handle\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {image_path}: {e}\")\n",
    "            # Create blank image if loading fails\n",
    "            image = Image.new('RGB', (96, 96), (0, 0, 0))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def get_sample_for_split(self, train_ratio=0.7, val_ratio=0.15):\n",
    "        \"\"\"Create train/val/test splits without loading all data.\"\"\"\n",
    "        if self._image_paths is None:\n",
    "            self._load_paths_lazy()\n",
    "            \n",
    "        total_samples = len(self._image_paths)\n",
    "        indices = list(range(total_samples))\n",
    "        \n",
    "        # Shuffle for random splits\n",
    "        import random\n",
    "        random.seed(42)\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        # Calculate split sizes\n",
    "        train_size = int(train_ratio * total_samples)\n",
    "        val_size = int(val_ratio * total_samples)\n",
    "        \n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:train_size + val_size]\n",
    "        test_indices = indices[train_size + val_size:]\n",
    "        \n",
    "        return train_indices, val_indices, test_indices\n",
    "\n",
    "print(\"Memory-optimized LazyUnifiedDataset class defined!\")\n",
    "print(\"Features: Lazy loading, minimal memory footprint, efficient splitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient data loading with smaller batch size and gradient accumulation\n",
    "from torchvision.transforms import AutoAugment, AutoAugmentPolicy\n",
    "import gc\n",
    "\n",
    "# Reduced transforms to save memory during augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.RandomResizedCrop(96, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),  # Reduced from 20\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Reduced\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.05, scale=(0.02, 0.20))  # Reduced probability and scale\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create lazy dataset - paths not loaded until first access\n",
    "print(\"Creating lazy dataset...\")\n",
    "full_dataset = LazyUnifiedDataset(dataset_info, transform=transform_train)\n",
    "\n",
    "# Get splits without loading all data\n",
    "train_indices, val_indices, test_indices = full_dataset.get_sample_for_split()\n",
    "\n",
    "print(f\"Dataset splits (calculated efficiently):\")\n",
    "print(f\"  Total images: {len(full_dataset)}\")\n",
    "print(f\"  Training: {len(train_indices)}\")\n",
    "print(f\"  Validation: {len(val_indices)}\")\n",
    "print(f\"  Test: {len(test_indices)}\")\n",
    "\n",
    "# Create separate dataset instances for each split\n",
    "train_dataset = LazyUnifiedDataset(\n",
    "    dataset_info, \n",
    "    subset_indices=train_indices,\n",
    "    transform=transform_train\n",
    ")\n",
    "\n",
    "val_dataset = LazyUnifiedDataset(\n",
    "    dataset_info, \n",
    "    subset_indices=val_indices,\n",
    "    transform=transform_val\n",
    ")\n",
    "\n",
    "test_dataset = LazyUnifiedDataset(\n",
    "    dataset_info, \n",
    "    subset_indices=test_indices,\n",
    "    transform=transform_val\n",
    ")\n",
    "\n",
    "# Reduced batch size for memory efficiency + gradient accumulation\n",
    "batch_size = 8  # Reduced from 16 to handle memory better\n",
    "num_workers = 1  # Reduced workers to save memory\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=False,  # Disable pin_memory to save GPU memory\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "print(f\"\\nMemory-optimized DataLoaders created:\")\n",
    "print(f\"  Batch size: {batch_size} (with gradient accumulation)\")\n",
    "print(f\"  Workers: {num_workers}\")\n",
    "print(f\"  Pin memory: False (saves GPU memory)\")\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "print(f\"  Test samples: {len(test_dataset)}\")\n",
    "print(f\"  Number of classes: {len(dataset_info['class_names'])}\")\n",
    "\n",
    "# Test loading a single batch to verify everything works\n",
    "print(f\"\\nTesting memory-efficient data loading...\")\n",
    "try:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"✅ Successfully loaded batch: {sample_batch[0].shape}, {sample_batch[1].shape}\")\n",
    "    \n",
    "    # Calculate approximate memory usage\n",
    "    batch_memory_mb = (sample_batch[0].numel() * 4) / (1024 * 1024)  # 4 bytes per float32\n",
    "    print(f\"✅ Batch memory usage: ~{batch_memory_mb:.1f} MB\")\n",
    "    \n",
    "    # Free the test batch\n",
    "    del sample_batch\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in data loading: {e}\")\n",
    "\n",
    "# Aggressive memory cleanup\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(f\"✅ Memory-efficient data loading setup completed\")\n",
    "print(f\"Classes: {dataset_info['class_names'][:5]}... ({len(dataset_info['class_names'])} total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Self-attention mechanism for enhanced feature representation.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        # Channel attention\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // 8, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_channels // 8, in_channels, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Spatial attention\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
    "        \n",
    "        # Learnable attention weight\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Channel attention\n",
    "        avg_out = self.fc(self.avg_pool(x).view(x.size(0), -1))\n",
    "        max_out = self.fc(self.max_pool(x).view(x.size(0), -1))\n",
    "        channel_att = self.sigmoid(avg_out + max_out).view(x.size(0), x.size(1), 1, 1)\n",
    "        \n",
    "        # Apply channel attention\n",
    "        x_channel = x * channel_att\n",
    "        \n",
    "        # Spatial attention\n",
    "        avg_out = torch.mean(x_channel, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x_channel, dim=1, keepdim=True)\n",
    "        spatial_att = self.sigmoid(self.conv(torch.cat([avg_out, max_out], dim=1)))\n",
    "        \n",
    "        # Apply spatial attention with learnable weight\n",
    "        out = x + self.gamma * (x_channel * spatial_att)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with batch normalization and attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                              stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                              stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        self.dropout = nn.Dropout2d(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class DeepLearningV2(nn.Module):\n",
    "    \"\"\"Advanced CNN with ResNet architecture, attention mechanisms, and modern techniques.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(DeepLearningV2, self).__init__()\n",
    "        \n",
    "        # Initial convolution layer\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
    "        \n",
    "        # Attention mechanisms\n",
    "        self.attention1 = AttentionBlock(128)\n",
    "        self.attention2 = AttentionBlock(256)\n",
    "        self.attention3 = AttentionBlock(512)\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # FIXED: Replace BatchNorm1d with LayerNorm to handle batch_size=1\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LayerNorm(512),  # Changed from BatchNorm1d to LayerNorm\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),  # Changed from BatchNorm1d to LayerNorm\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride=1):\n",
    "        \"\"\"Create a residual layer.\"\"\"\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride, downsample))\n",
    "        \n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):  # Added LayerNorm initialization\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # Residual layers with attention\n",
    "        x = self.layer1(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = self.attention1(x)  # Apply attention after layer2\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        x = self.attention2(x)  # Apply attention after layer3\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "        x = self.attention3(x)  # Apply attention after layer4\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_feature_maps(self, x):\n",
    "        \"\"\"Extract feature maps at different levels for visualization.\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Initial layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        features['conv1'] = x\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # Residual layers\n",
    "        x = self.layer1(x)\n",
    "        features['layer1'] = x\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        features['layer2'] = x\n",
    "        x = self.attention1(x)\n",
    "        features['attention1'] = x\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        features['layer3'] = x\n",
    "        x = self.attention2(x)\n",
    "        features['attention2'] = x\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "        features['layer4'] = x\n",
    "        x = self.attention3(x)\n",
    "        features['attention3'] = x\n",
    "        \n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance with memory optimizations\n",
    "num_classes = len(dataset_info['class_names'])\n",
    "\n",
    "# Add memory monitoring\n",
    "def print_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3   # GB\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n",
    "    \n",
    "    import psutil\n",
    "    process = psutil.Process()\n",
    "    ram_usage = process.memory_info().rss / 1024**3  # GB\n",
    "    print(f\"RAM Usage: {ram_usage:.2f}GB\")\n",
    "\n",
    "print(f\"Creating model for {num_classes} classes...\")\n",
    "print_memory_usage()\n",
    "\n",
    "model = DeepLearningV2(num_classes=num_classes).to(device)\n",
    "\n",
    "print(f\"Model created for {num_classes} classes\")\n",
    "print(f\"Classes (sample): {dataset_info['class_names'][:5]}...\")\n",
    "\n",
    "# Memory cleanup after model creation\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print_memory_usage()\n",
    "\n",
    "# Print model summary with reduced batch size\n",
    "print(\"\\nModel Summary:\")\n",
    "try:\n",
    "    print(summary(model, input_size=(4, 3, 96, 96), device=str(device)))  # Reduced batch size for summary\n",
    "except Exception as e:\n",
    "    print(f\"Summary generation failed (memory): {e}\")\n",
    "    # Manual parameter count\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Model size: ~{total_params * 4 / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Training with Modern Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEfficientTrainingManager:\n",
    "    \"\"\"Memory-efficient training manager with gradient accumulation and monitoring.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, class_names, accumulation_steps=4):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.class_names = class_names\n",
    "        self.accumulation_steps = accumulation_steps  # Simulate larger batch size\n",
    "        \n",
    "        # Training history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        self.learning_rates = []\n",
    "        \n",
    "        # Best model tracking\n",
    "        self.best_val_accuracy = 0.0\n",
    "        self.best_model_state = None\n",
    "        self.patience_counter = 0\n",
    "        \n",
    "        # Memory monitoring\n",
    "        self.memory_usage = []\n",
    "    \n",
    "    def monitor_memory(self):\n",
    "        \"\"\"Monitor memory usage.\"\"\"\n",
    "        memory_info = {}\n",
    "        if torch.cuda.is_available():\n",
    "            memory_info['gpu_allocated'] = torch.cuda.memory_allocated() / 1024**3\n",
    "            memory_info['gpu_reserved'] = torch.cuda.memory_reserved() / 1024**3\n",
    "        \n",
    "        import psutil\n",
    "        process = psutil.Process()\n",
    "        memory_info['ram_usage'] = process.memory_info().rss / 1024**3\n",
    "        \n",
    "        return memory_info\n",
    "    \n",
    "    def mixup_criterion(self, pred, y_a, y_b, lam):\n",
    "        \"\"\"Mixup loss function.\"\"\"\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "    \n",
    "    def train_epoch_memory_efficient(self, train_loader, criterion, optimizer, use_mixup=True, mixup_alpha=0.2):\n",
    "        \"\"\"Memory-efficient training with gradient accumulation.\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            \n",
    "            # Apply mixup less frequently to save memory\n",
    "            if use_mixup and np.random.random() < 0.3:  # Reduced from 0.5\n",
    "                mixed_data, y_a, y_b, lam = self._mixup_data(data, target, mixup_alpha)\n",
    "                \n",
    "                output = self.model(mixed_data)\n",
    "                loss = self.mixup_criterion(output, y_a, y_b, lam)\n",
    "                \n",
    "                # Scale loss for gradient accumulation\n",
    "                loss = loss / self.accumulation_steps\n",
    "                loss.backward()\n",
    "                \n",
    "                running_loss += loss.item() * self.accumulation_steps\n",
    "                \n",
    "                # Accuracy calculation for mixup is approximate\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (lam * (predicted == y_a).sum().item() + \n",
    "                           (1 - lam) * (predicted == y_b).sum().item())\n",
    "            \n",
    "            else:\n",
    "                # Standard training\n",
    "                output = self.model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                # Scale loss for gradient accumulation\n",
    "                loss = loss / self.accumulation_steps\n",
    "                loss.backward()\n",
    "                \n",
    "                running_loss += loss.item() * self.accumulation_steps\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "            \n",
    "            # Gradient accumulation step\n",
    "            if (batch_idx + 1) % self.accumulation_steps == 0:\n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Memory cleanup every few steps\n",
    "                if (batch_idx + 1) % (self.accumulation_steps * 4) == 0:\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "            \n",
    "            # Progress reporting (less frequent)\n",
    "            if batch_idx % 100 == 0:\n",
    "                memory_info = self.monitor_memory()\n",
    "                print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item() * self.accumulation_steps:.4f}, '\n",
    "                      f'GPU: {memory_info.get(\"gpu_allocated\", 0):.1f}GB')\n",
    "        \n",
    "        # Final gradient step if needed\n",
    "        if len(train_loader) % self.accumulation_steps != 0:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = 100. * correct / total\n",
    "        \n",
    "        self.train_losses.append(epoch_loss)\n",
    "        self.train_accuracies.append(epoch_accuracy)\n",
    "        \n",
    "        return epoch_loss, epoch_accuracy\n",
    "    \n",
    "    def _mixup_data(self, x, y, alpha=0.2):  # Reduced alpha for less mixing\n",
    "        \"\"\"Apply mixup augmentation.\"\"\"\n",
    "        if alpha > 0:\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "        else:\n",
    "            lam = 1\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        index = torch.randperm(batch_size).to(x.device)\n",
    "        \n",
    "        mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "        y_a, y_b = y, y[index]\n",
    "        \n",
    "        return mixed_x, y_a, y_b, lam\n",
    "    \n",
    "    def validate_epoch(self, val_loader, criterion):\n",
    "        \"\"\"Memory-efficient validation epoch.\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(val_loader):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "                \n",
    "                # Memory cleanup during validation\n",
    "                if batch_idx % 20 == 0 and torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "        \n",
    "        epoch_loss = running_loss / len(val_loader)\n",
    "        epoch_accuracy = 100. * correct / total\n",
    "        \n",
    "        self.val_losses.append(epoch_loss)\n",
    "        self.val_accuracies.append(epoch_accuracy)\n",
    "        \n",
    "        # Early stopping and best model saving\n",
    "        if epoch_accuracy > self.best_val_accuracy:\n",
    "            self.best_val_accuracy = epoch_accuracy\n",
    "            self.best_model_state = self.model.state_dict().copy()\n",
    "            self.patience_counter = 0\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "        \n",
    "        return epoch_loss, epoch_accuracy\n",
    "    \n",
    "    def train_memory_efficient(self, train_loader, val_loader, num_epochs, patience=8):\n",
    "        \"\"\"Memory-efficient training loop.\"\"\"\n",
    "        print(f\"Starting memory-efficient training for {num_epochs} epochs...\")\n",
    "        print(f\"Gradient accumulation steps: {self.accumulation_steps}\")\n",
    "        print(f\"Effective batch size: {train_loader.batch_size * self.accumulation_steps}\")\n",
    "        print(f\"Training on {len(self.class_names)} classes\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Optimizers with reduced learning rate for stability\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "        \n",
    "        # Simpler scheduler to save memory\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "        \n",
    "        # Label smoothing loss\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.05)  # Reduced smoothing\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            print(\"-\" * 20)\n",
    "            \n",
    "            # Monitor memory at start of epoch\n",
    "            memory_info = self.monitor_memory()\n",
    "            self.memory_usage.append(memory_info)\n",
    "            print(f\"Memory at epoch start: GPU {memory_info.get('gpu_allocated', 0):.1f}GB, \"\n",
    "                  f\"RAM {memory_info.get('ram_usage', 0):.1f}GB\")\n",
    "            \n",
    "            # Training with memory efficiency\n",
    "            train_loss, train_acc = self.train_epoch_memory_efficient(\n",
    "                train_loader, criterion, optimizer, use_mixup=True\n",
    "            )\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc = self.validate_epoch(val_loader, criterion)\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step()\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            self.learning_rates.append(current_lr)\n",
    "            \n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "            print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "            print(f\"Best Val Acc: {self.best_val_accuracy:.2f}%\")\n",
    "            \n",
    "            # Aggressive memory cleanup after each epoch\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "            \n",
    "            # Plot progress every 3 epochs (less frequent)\n",
    "            if (epoch + 1) % 3 == 0:\n",
    "                self.plot_training_progress()\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "        print(f\"Best validation accuracy: {self.best_val_accuracy:.2f}%\")\n",
    "        \n",
    "        # Load best model\n",
    "        if self.best_model_state:\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "        \n",
    "        return self.best_val_accuracy\n",
    "    \n",
    "    def evaluate_model_advanced(self, test_loader):\n",
    "        \"\"\"Advanced model evaluation with detailed metrics.\"\"\"\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        all_probabilities = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.model(data)\n",
    "                probabilities = F.softmax(output, dim=1)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                \n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "                \n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(target.cpu().numpy())\n",
    "                all_probabilities.extend(probabilities.cpu().numpy())\n",
    "        \n",
    "        test_accuracy = 100. * correct / total\n",
    "        \n",
    "        print(f\"\\nAdvanced Test Evaluation:\")\n",
    "        print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "        \n",
    "        # Detailed classification report (truncated for many classes)\n",
    "        print(\"\\nClassification Report (first 10 classes):\")\n",
    "        unique_classes = sorted(list(set(all_targets)))\n",
    "        display_classes = unique_classes[:10]\n",
    "        \n",
    "        if len(display_classes) < len(unique_classes):\n",
    "            print(f\"Note: Showing first 10 of {len(unique_classes)} classes\")\n",
    "        \n",
    "        from sklearn.metrics import classification_report\n",
    "        print(classification_report(all_targets, all_predictions, \n",
    "                                  target_names=[self.class_names[i] for i in display_classes],\n",
    "                                  labels=display_classes, digits=4))\n",
    "        \n",
    "        return test_accuracy, all_predictions, all_targets, all_probabilities\n",
    "    \n",
    "    def plot_training_progress(self):\n",
    "        \"\"\"Plot training progress with memory usage.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        \n",
    "        epochs = range(1, len(self.train_losses) + 1)\n",
    "        \n",
    "        # Loss plot\n",
    "        axes[0, 0].plot(epochs, self.train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "        axes[0, 0].plot(epochs, self.val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "        axes[0, 0].set_title('Loss Progress')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        axes[0, 1].plot(epochs, self.train_accuracies, 'b-', label='Training Accuracy', linewidth=2)\n",
    "        axes[0, 1].plot(epochs, self.val_accuracies, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "        axes[0, 1].set_title('Accuracy Progress')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning rate plot\n",
    "        if self.learning_rates:\n",
    "            axes[1, 0].plot(epochs, self.learning_rates, 'g-', linewidth=2)\n",
    "            axes[1, 0].set_title('Learning Rate Schedule')\n",
    "            axes[1, 0].set_xlabel('Epoch')\n",
    "            axes[1, 0].set_ylabel('Learning Rate')\n",
    "            axes[1, 0].set_yscale('log')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Memory usage plot\n",
    "        if self.memory_usage:\n",
    "            gpu_memory = [m.get('gpu_allocated', 0) for m in self.memory_usage]\n",
    "            ram_memory = [m.get('ram_usage', 0) for m in self.memory_usage]\n",
    "            \n",
    "            memory_epochs = range(1, len(self.memory_usage) + 1)\n",
    "            axes[1, 1].plot(memory_epochs, gpu_memory, 'orange', label='GPU Memory (GB)', linewidth=2)\n",
    "            axes[1, 1].plot(memory_epochs, ram_memory, 'purple', label='RAM Usage (GB)', linewidth=2)\n",
    "            axes[1, 1].set_title('Memory Usage')\n",
    "            axes[1, 1].set_xlabel('Epoch')\n",
    "            axes[1, 1].set_ylabel('Memory (GB)')\n",
    "            axes[1, 1].legend()\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"✅ Memory-efficient training manager created with evaluation method!\")\n",
    "print(\"Features: Gradient accumulation, memory monitoring, advanced evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with Advanced Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create memory-efficient training manager with gradient accumulation\n",
    "memory_trainer = MemoryEfficientTrainingManager(\n",
    "    model, \n",
    "    device, \n",
    "    dataset_info['class_names'], \n",
    "    accumulation_steps=4  # Effective batch size = 8 * 4 = 32\n",
    ")\n",
    "\n",
    "# Train with reduced epochs and more conservative settings\n",
    "num_epochs = 25  # Reduced from 50\n",
    "patience = 8     # Reduced from 15\n",
    "\n",
    "print(\"Starting memory-efficient training...\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Accumulation steps: {memory_trainer.accumulation_steps}\")\n",
    "print(f\"Effective batch size: {batch_size * memory_trainer.accumulation_steps}\")\n",
    "print(f\"Total classes: {len(dataset_info['class_names'])}\")\n",
    "\n",
    "# Initial memory check\n",
    "initial_memory = memory_trainer.monitor_memory()\n",
    "print(f\"Initial memory: GPU {initial_memory.get('gpu_allocated', 0):.1f}GB, \"\n",
    "      f\"RAM {initial_memory.get('ram_usage', 0):.1f}GB\")\n",
    "\n",
    "try:\n",
    "    best_val_accuracy = memory_trainer.train_memory_efficient(\n",
    "        train_loader, val_loader, num_epochs, patience\n",
    "    )\n",
    "    \n",
    "    # Final training progress plot\n",
    "    memory_trainer.plot_training_progress()\n",
    "    \n",
    "    print(f\"\\n✅ Training completed successfully!\")\n",
    "    print(f\"Best validation accuracy: {best_val_accuracy:.2f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Training failed due to memory issues: {e}\")\n",
    "    print(\"Try reducing batch size further or using a smaller model.\")\n",
    "    \n",
    "    # Emergency memory cleanup\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Model Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model evaluation\n",
    "# Fixed: Use the correct trainer variable name\n",
    "try:\n",
    "    test_accuracy, predictions, targets, probabilities = memory_trainer.evaluate_model_advanced(test_loader)\n",
    "except AttributeError:\n",
    "    # If evaluate_model_advanced doesn't exist, create a simple evaluation\n",
    "    print(\"Creating simple model evaluation...\")\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            probabilities_batch = F.softmax(output, dim=1)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            \n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities_batch.cpu().numpy())\n",
    "    \n",
    "    test_accuracy = 100. * correct / total\n",
    "    predictions = all_predictions\n",
    "    targets = all_targets\n",
    "    probabilities = all_probabilities\n",
    "    \n",
    "    print(f\"\\nTest Evaluation Results:\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    # Detailed classification report (first 10 classes)\n",
    "    unique_classes = sorted(list(set(targets)))\n",
    "    display_classes = unique_classes[:10]\n",
    "    \n",
    "    if len(display_classes) < len(unique_classes):\n",
    "        print(f\"Classification Report (showing first 10 of {len(unique_classes)} classes):\")\n",
    "    \n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(targets, predictions, \n",
    "                              target_names=[dataset_info['class_names'][i] for i in display_classes],\n",
    "                              labels=display_classes, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced visualization and analysis\n",
    "def analyze_model_confidence(probabilities, predictions, targets, class_names):\n",
    "    \"\"\"Analyze model confidence and prediction quality.\"\"\"\n",
    "    probabilities = np.array(probabilities)\n",
    "    predictions = np.array(predictions)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    # Calculate confidence (max probability)\n",
    "    confidences = np.max(probabilities, axis=1)\n",
    "    correct_mask = predictions == targets\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Confidence distribution\n",
    "    axes[0, 0].hist(confidences[correct_mask], bins=30, alpha=0.7, label='Correct', color='green')\n",
    "    axes[0, 0].hist(confidences[~correct_mask], bins=30, alpha=0.7, label='Incorrect', color='red')\n",
    "    axes[0, 0].set_title('Confidence Distribution')\n",
    "    axes[0, 0].set_xlabel('Confidence')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Accuracy vs Confidence\n",
    "    confidence_bins = np.linspace(0, 1, 11)\n",
    "    bin_accuracies = []\n",
    "    bin_centers = []\n",
    "    \n",
    "    for i in range(len(confidence_bins) - 1):\n",
    "        mask = (confidences >= confidence_bins[i]) & (confidences < confidence_bins[i + 1])\n",
    "        if np.sum(mask) > 0:\n",
    "            accuracy = np.mean(correct_mask[mask])\n",
    "            bin_accuracies.append(accuracy)\n",
    "            bin_centers.append((confidence_bins[i] + confidence_bins[i + 1]) / 2)\n",
    "    \n",
    "    axes[0, 1].plot(bin_centers, bin_accuracies, 'bo-')\n",
    "    axes[0, 1].plot([0, 1], [0, 1], 'r--', label='Perfect Calibration')\n",
    "    axes[0, 1].set_title('Calibration Plot')\n",
    "    axes[0, 1].set_xlabel('Confidence')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Per-class confidence\n",
    "    class_confidences = []\n",
    "    for i in range(len(class_names)):\n",
    "        class_mask = targets == i\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_confidences.append(confidences[class_mask])\n",
    "        else:\n",
    "            class_confidences.append([])\n",
    "    \n",
    "    axes[1, 0].boxplot(class_confidences, labels=class_names)\n",
    "    axes[1, 0].set_title('Confidence by Class')\n",
    "    axes[1, 0].set_ylabel('Confidence')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Top-k accuracy\n",
    "    k_values = range(1, min(6, len(class_names) + 1))\n",
    "    top_k_accuracies = []\n",
    "    \n",
    "    for k in k_values:\n",
    "        top_k_pred = np.argsort(probabilities, axis=1)[:, -k:]\n",
    "        top_k_correct = np.any(top_k_pred == targets[:, np.newaxis], axis=1)\n",
    "        top_k_accuracies.append(np.mean(top_k_correct) * 100)\n",
    "    \n",
    "    axes[1, 1].bar(k_values, top_k_accuracies)\n",
    "    axes[1, 1].set_title('Top-k Accuracy')\n",
    "    axes[1, 1].set_xlabel('k')\n",
    "    axes[1, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(top_k_accuracies):\n",
    "        axes[1, 1].text(i + 1, v + 1, f'{v:.1f}%', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print confidence statistics\n",
    "    print(f\"\\nConfidence Analysis:\")\n",
    "    print(f\"Average confidence (correct): {np.mean(confidences[correct_mask]):.3f}\")\n",
    "    print(f\"Average confidence (incorrect): {np.mean(confidences[~correct_mask]):.3f}\")\n",
    "    print(f\"Top-1 Accuracy: {top_k_accuracies[0]:.2f}%\")\n",
    "    if len(top_k_accuracies) > 2:\n",
    "        print(f\"Top-3 Accuracy: {top_k_accuracies[2]:.2f}%\")\n",
    "\n",
    "analyze_model_confidence(probabilities, predictions, targets, full_dataset.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention maps\n",
    "def visualize_attention_maps(model, test_loader, device, class_names):\n",
    "    \"\"\"Visualize attention mechanisms.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch for visualization\n",
    "    data_iter = iter(test_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    images = images.to(device)\n",
    "    \n",
    "    # Hook to capture attention weights\n",
    "    attention_weights = {}\n",
    "    \n",
    "    def hook_fn(name):\n",
    "        def hook(module, input, output):\n",
    "            if hasattr(module, 'gamma'):\n",
    "                attention_weights[name] = module.gamma.item()\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks\n",
    "    handles = []\n",
    "    handles.append(model.attention1.register_forward_hook(hook_fn('attention1')))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    # Denormalize images\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "    images_denorm = images * std + mean\n",
    "    images_denorm = torch.clamp(images_denorm, 0, 1)\n",
    "    \n",
    "    for i in range(min(4, len(images))):\n",
    "        img = images_denorm[i].cpu().permute(1, 2, 0)\n",
    "        true_label = class_names[labels[i]]\n",
    "        pred_label = class_names[predicted[i]]\n",
    "        confidence = probabilities[i][predicted[i]].item()\n",
    "        \n",
    "        # Original image\n",
    "        axes[0, i].imshow(img)\n",
    "        color = 'green' if labels[i] == predicted[i] else 'red'\n",
    "        axes[0, i].set_title(\n",
    "            f'True: {true_label}\\nPred: {pred_label}\\nConf: {confidence:.3f}',\n",
    "            color=color\n",
    "        )\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Attention visualization (simplified)\n",
    "        # In a real implementation, you would extract and visualize actual attention maps\n",
    "        axes[1, i].imshow(img)\n",
    "        axes[1, i].set_title(f'Attention Map\\n(Simplified)')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Model Predictions with Attention Analysis', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if attention_weights:\n",
    "        print(f\"\\nAttention Weights:\")\n",
    "        for name, weight in attention_weights.items():\n",
    "            print(f\"{name}: {weight:.4f}\")\n",
    "\n",
    "visualize_attention_maps(model, test_loader, device, full_dataset.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Integration and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLearningV2Classifier(BaseImageClassifier):\n",
    "    \"\"\"Deep Learning v2 classifier implementing the base interface.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"deep-learning-v2\", version=\"2.0.0\"):\n",
    "        super().__init__(model_name, version)\n",
    "        self.model = None\n",
    "        self.class_names = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((96, 96)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def load_model(self, model_path: str) -> None:\n",
    "        \"\"\"Load the trained model.\"\"\"\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        \n",
    "        # Recreate model architecture\n",
    "        num_classes = len(checkpoint['class_names'])\n",
    "        self.model = DeepLearningV2(num_classes=num_classes)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.class_names = checkpoint['class_names']\n",
    "        self._is_loaded = True\n",
    "    \n",
    "    def preprocess(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Preprocess image for prediction.\"\"\"\n",
    "        # Convert numpy array to PIL Image\n",
    "        if image.dtype != np.uint8:\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "        \n",
    "        pil_image = Image.fromarray(image)\n",
    "        if pil_image.mode != 'RGB':\n",
    "            pil_image = pil_image.convert('RGB')\n",
    "        \n",
    "        # Apply transforms\n",
    "        tensor_image = self.transform(pil_image)\n",
    "        \n",
    "        return tensor_image\n",
    "    \n",
    "    def predict(self, image: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Make predictions on input image.\"\"\"\n",
    "        if not self.is_loaded:\n",
    "            raise ValueError(\"Model not loaded. Call load_model() first.\")\n",
    "        \n",
    "        # Preprocess image\n",
    "        tensor_image = self.preprocess(image)\n",
    "        tensor_image = tensor_image.unsqueeze(0).to(self.device)  # Add batch dimension\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(tensor_image)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        # Convert to class name mapping\n",
    "        predictions = {}\n",
    "        for i, prob in enumerate(probabilities[0]):\n",
    "            predictions[self.class_names[i]] = float(prob.cpu())\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def get_metadata(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get model metadata.\"\"\"\n",
    "        return {\n",
    "            \"model_type\": \"deep_learning_v2\",\n",
    "            \"architecture\": \"Advanced CNN with ResNet + Attention\",\n",
    "            \"input_size\": \"96x96x3\",\n",
    "            \"features\": [\"residual_connections\", \"self_attention\", \"channel_attention\", \"mixup\", \"label_smoothing\"],\n",
    "            \"classes\": self.class_names,\n",
    "            \"parameters\": sum(p.numel() for p in self.model.parameters()) if self.model else 0,\n",
    "            \"device\": str(self.device),\n",
    "            \"version\": self.version\n",
    "        }\n",
    "    \n",
    "    def save_model(self, model_path: str, model, class_names, accuracy, training_history):\n",
    "        \"\"\"Save the trained model.\"\"\"\n",
    "        checkpoint = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'class_names': class_names,\n",
    "            'accuracy': accuracy,\n",
    "            'training_history': training_history,\n",
    "            'model_config': {\n",
    "                'num_classes': len(class_names),\n",
    "                'input_size': (96, 96),\n",
    "                'architecture': 'DeepLearningV2',\n",
    "                'features': ['residual_connections', 'attention_mechanisms', 'advanced_training']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, model_path)\n",
    "        print(f\"Advanced model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the advanced trained model\n",
    "deep_v2_classifier = DeepLearningV2Classifier()\n",
    "\n",
    "# Prepare advanced training history using memory_trainer\n",
    "training_history = {\n",
    "    'train_losses': memory_trainer.train_losses,\n",
    "    'val_losses': memory_trainer.val_losses,\n",
    "    'train_accuracies': memory_trainer.train_accuracies,\n",
    "    'val_accuracies': memory_trainer.val_accuracies,\n",
    "    'learning_rates': memory_trainer.learning_rates,\n",
    "    'best_val_accuracy': memory_trainer.best_val_accuracy\n",
    "}\n",
    "\n",
    "# Save model\n",
    "model_path = \"../models/deep_v2_classifier.pth\"\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "deep_v2_classifier.save_model(\n",
    "    model_path, model, dataset_info['class_names'], test_accuracy, training_history\n",
    ")\n",
    "\n",
    "# Test the saved model\n",
    "test_classifier = DeepLearningV2Classifier()\n",
    "test_classifier.load_model(model_path)\n",
    "\n",
    "# Test prediction on a sample image\n",
    "sample_batch = next(iter(test_loader))\n",
    "sample_image = sample_batch[0][0]  # Get first image from batch\n",
    "sample_label = sample_batch[1][0]  # Get corresponding label\n",
    "\n",
    "# Convert tensor back to numpy for prediction\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "sample_image_denorm = sample_image * std + mean\n",
    "sample_image_denorm = torch.clamp(sample_image_denorm, 0, 1)\n",
    "sample_image_np = (sample_image_denorm.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "\n",
    "predictions = test_classifier.predict(sample_image_np)\n",
    "print(f\"\\nAdvanced model sample prediction: {predictions}\")\n",
    "print(f\"Actual class: {dataset_info['class_names'][sample_label]}\")\n",
    "\n",
    "# Register model in registry\n",
    "registry = ModelRegistry()\n",
    "metadata = ModelMetadata(\n",
    "    name=\"deep-learning-v2\",\n",
    "    version=\"2.0.0\",\n",
    "    model_type=\"deep_v2\",\n",
    "    accuracy=test_accuracy / 100.0,  # Convert percentage to decimal\n",
    "    training_date=\"2024-01-01\",\n",
    "    model_path=model_path,\n",
    "    config={\n",
    "        \"architecture\": \"Advanced CNN with ResNet + Attention\",\n",
    "        \"num_classes\": len(dataset_info['class_names']),\n",
    "        \"input_size\": \"96x96x3\",\n",
    "        \"epochs_trained\": len(memory_trainer.train_losses),\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"learning_rate_schedule\": \"StepLR\",\n",
    "        \"techniques\": [\"mixup\", \"label_smoothing\", \"attention\", \"residual_connections\"]\n",
    "    },\n",
    "    performance_metrics={\n",
    "        \"test_accuracy\": test_accuracy / 100.0,\n",
    "        \"best_val_accuracy\": memory_trainer.best_val_accuracy / 100.0,\n",
    "        \"final_train_loss\": memory_trainer.train_losses[-1],\n",
    "        \"final_val_loss\": memory_trainer.val_losses[-1],\n",
    "        \"model_parameters\": sum(p.numel() for p in model.parameters())\n",
    "    }\n",
    ")\n",
    "\n",
    "registry.register_model(metadata)\n",
    "print(f\"\\nAdvanced model registered with test accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"Total classes trained on: {len(dataset_info['class_names'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with previous models\n",
    "def compare_all_models():\n",
    "    \"\"\"Compare performance across all model versions.\"\"\"\n",
    "    registry = ModelRegistry()\n",
    "    \n",
    "    models_comparison = []\n",
    "    \n",
    "    # Get all registered models\n",
    "    all_models = registry.list_models()\n",
    "    \n",
    "    for model_name in all_models:\n",
    "        model_info = registry.get_model(model_name)\n",
    "        if model_info:\n",
    "            models_comparison.append({\n",
    "                'Model': model_name,\n",
    "                'Type': model_info.model_type,\n",
    "                'Accuracy': model_info.accuracy * 100,\n",
    "                'Parameters': model_info.performance_metrics.get('model_parameters', 'N/A')\n",
    "            })\n",
    "    \n",
    "    if models_comparison:\n",
    "        import pandas as pd\n",
    "        \n",
    "        df = pd.DataFrame(models_comparison)\n",
    "        df = df.sort_values('Accuracy', ascending=False)\n",
    "        \n",
    "        print(\"\\nModel Performance Comparison:\")\n",
    "        print(df.to_string(index=False))\n",
    "        \n",
    "        # Plot comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold', 'plum']\n",
    "        bars = plt.bar(df['Model'], df['Accuracy'], color=colors[:len(df)])\n",
    "        \n",
    "        plt.title('Model Performance Comparison', fontsize=16)\n",
    "        plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "        plt.xlabel('Model', fontsize=12)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylim(0, 100)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars, df['Accuracy']):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Performance improvements\n",
    "        if len(df) > 1:\n",
    "            best_acc = df.iloc[0]['Accuracy']\n",
    "            baseline_acc = df.iloc[-1]['Accuracy']\n",
    "            improvement = best_acc - baseline_acc\n",
    "            \n",
    "            print(f\"\\nPerformance Analysis:\")\n",
    "            print(f\"Best Model: {df.iloc[0]['Model']} ({best_acc:.2f}%)\")\n",
    "            print(f\"Baseline: {df.iloc[-1]['Model']} ({baseline_acc:.2f}%)\")\n",
    "            print(f\"Total Improvement: {improvement:.2f} percentage points\")\n",
    "            print(f\"Relative Improvement: {(improvement/baseline_acc)*100:.1f}%\")\n",
    "    \n",
    "    else:\n",
    "        print(\"No models found for comparison.\")\n",
    "\n",
    "compare_all_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Insights\n",
    "\n",
    "### Advanced Architecture Features:\n",
    "- **Residual Connections**: Enable training of deeper networks without vanishing gradients\n",
    "- **Attention Mechanisms**: Self-attention and channel attention for feature enhancement\n",
    "- **Advanced Normalization**: Batch normalization for training stability\n",
    "- **Sophisticated Classifier**: Multi-layer classifier with progressive dimension reduction\n",
    "\n",
    "### Advanced Training Techniques:\n",
    "- **Mixup Augmentation**: Improves generalization by training on mixed samples\n",
    "- **Label Smoothing**: Prevents overconfident predictions and improves calibration\n",
    "- **AdamW Optimizer**: Better weight decay handling than standard Adam\n",
    "- **Cosine Annealing**: Periodic learning rate restarts for better convergence\n",
    "- **Gradient Clipping**: Prevents exploding gradients in deep networks\n",
    "- **Early Stopping**: Automatic training termination to prevent overfitting\n",
    "\n",
    "### Enhanced Data Processing:\n",
    "- **AutoAugment**: Automatically learned data augmentation policies\n",
    "- **Advanced Transforms**: More sophisticated image preprocessing\n",
    "- **Larger Input Size**: 96x96 instead of 64x64 for more detail\n",
    "- **Random Erasing**: Additional regularization technique\n",
    "\n",
    "### Key Improvements Over v1:\n",
    "1. **Architecture Depth**: More sophisticated feature extraction\n",
    "2. **Attention Mechanisms**: Better focus on important features\n",
    "3. **Training Stability**: More robust training with modern techniques\n",
    "4. **Generalization**: Better performance on unseen data\n",
    "5. **Calibration**: More reliable confidence estimates\n",
    "\n",
    "### Performance Analysis:\n",
    "- **Accuracy**: Significant improvement over baseline models\n",
    "- **Confidence**: Better calibrated predictions\n",
    "- **Robustness**: More stable across different data distributions\n",
    "- **Efficiency**: Good balance between performance and computational cost\n",
    "\n",
    "### Production Considerations:\n",
    "- **Model Size**: Larger than v1 but still manageable for deployment\n",
    "- **Inference Time**: Reasonable for real-time applications\n",
    "- **Memory Usage**: Requires more GPU memory during training\n",
    "- **Scalability**: Architecture can be extended for more classes\n",
    "\n",
    "### Next Steps:\n",
    "1. **Transfer Learning**: Compare with pre-trained models\n",
    "2. **Ensemble Integration**: Combine with other model types\n",
    "3. **Hyperparameter Optimization**: Fine-tune for optimal performance\n",
    "4. **Production Optimization**: Model quantization and pruning\n",
    "\n",
    "### Technical Achievements:\n",
    "- Successfully implemented state-of-the-art deep learning techniques\n",
    "- Demonstrated significant performance improvements\n",
    "- Created a robust and well-calibrated model\n",
    "- Established a framework for advanced neural network development"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
