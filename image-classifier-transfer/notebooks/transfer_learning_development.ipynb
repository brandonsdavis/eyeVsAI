{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Image Classifier Development\n",
    "\n",
    "This notebook demonstrates the development of an image classifier using transfer learning with pre-trained models. We'll leverage established architectures trained on ImageNet and fine-tune them for our specific classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport os\nimport json\nfrom datetime import datetime\nimport sys\n\n# Add path to access extracted modules\nsys.path.append('../..')\nsys.path.append('../')\n\n# Import from extracted transfer learning modules\nfrom src.classifier import TransferLearningClassifier\nfrom src.trainer import TransferLearningTrainer\nfrom src.models import TransferLearningModel\nfrom src.config import TransferLearningClassifierConfig\nfrom src.data_loader import create_memory_efficient_datasets, discover_classes\n\n# Import from ml_models_core\nfrom ml_models_core.src.base_classifier import BaseImageClassifier\nfrom ml_models_core.src.model_registry import ModelRegistry, ModelMetadata"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-Efficient Data Loading\n",
    "\n",
    "**Note**: This notebook uses memory-efficient data loading similar to the deep learning notebooks. Instead of loading all images into memory at once, we:\n",
    "1. Store only file paths in memory\n",
    "2. Load images on-demand during training using TensorFlow's tf.data API\n",
    "3. Use data streaming and prefetching for optimal performance\n",
    "\n",
    "This approach allows training on large datasets without running out of memory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Configuration using extracted config module\nconfig = TransferLearningClassifierConfig(\n    base_model_name='resnet50',\n    image_size=(224, 224),\n    batch_size=32,\n    num_epochs=20,\n    learning_rate=1e-3,\n    fine_tune_layers=10,\n    fine_tune_learning_rate=1e-5,\n    dropout_rate=0.5,\n    validation_split=0.2,\n    dense_units=[512, 256],\n    mixed_precision=True,\n    use_xla=True,\n    class_weights=True,\n    cache_dataset=True\n)\n\nprint(\"Configuration created using extracted config module:\")\nprint(f\"Base model: {config.base_model_name}\")\nprint(f\"Image size: {config.image_size}\")\nprint(f\"Batch size: {config.batch_size}\")\nprint(f\"Mixed precision: {config.mixed_precision}\")\nprint(f\"XLA compilation: {config.use_xla}\")\n\n# Create memory-efficient datasets using extracted data loader\nprint(\"\\nCreating memory-efficient datasets...\")\n\n# Use existing dataset path\nfrom pathlib import Path\ndataset_path = Path(\"../../data/downloads/combined_unified_classification\")\n\nif not dataset_path.exists():\n    # Fallback to other available datasets\n    base_data_dir = Path(\"../../data/downloads\")\n    available_datasets = [\n        base_data_dir / \"combined_unified_classification\",\n        base_data_dir / \"oxford_pets\",\n        base_data_dir / \"vegetables\"\n    ]\n    \n    for candidate in available_datasets:\n        if candidate.exists():\n            dataset_path = candidate\n            break\n    else:\n        raise FileNotFoundError(\"No datasets found. Please run data preparation first.\")\n\nprint(f\"Dataset path: {dataset_path}\")\n\n# Create datasets using extracted data loader\ntrain_dataset, val_dataset, test_dataset, class_names, class_weights = create_memory_efficient_datasets(\n    str(dataset_path), config\n)\n\n# Update config with discovered classes\nconfig.num_classes = len(class_names)\n\nprint(f\"\\nMemory-efficient datasets created successfully!\")\nprint(f\"Training on {config.num_classes} classes\")\nprint(f\"Classes (first 10): {class_names[:10]}\")\nprint(f\"Class weights enabled: {config.class_weights}\")\nprint(f\"Dataset caching enabled: {config.cache_dataset}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Model Selection and Architecture"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Create transfer learning model using extracted modules\nprint(\"Creating transfer learning model using extracted modules...\")\n\n# Create model instance\ntransfer_model = TransferLearningModel(config, config.num_classes)\n\n# Build the model\nmodel = transfer_model.build_model()\n\nprint(f\"\\nModel created successfully:\")\nprint(f\"Base model: {config.base_model_name}\")\nprint(f\"Total parameters: {model.count_params():,}\")\nprint(f\"Trainable parameters: {sum(p.numel() for p in model.trainable_variables):,}\")\nprint(f\"Input shape: {model.input_shape}\")\nprint(f\"Output shape: {model.output_shape}\")\n\n# Display model architecture info\nmodel_info = transfer_model.get_model_info()\nprint(f\"\\nModel architecture features:\")\nfor feature in model_info.get('features', []):\n    if feature:\n        print(f\"  - {feature}\")\n\nprint(f\"\\nModel summary (first few layers):\")\nfor i, layer in enumerate(model.layers[:10]):\n    print(f\"  {i}: {layer.name} - {layer.__class__.__name__}\")\nif len(model.layers) > 10:\n    print(f\"  ... and {len(model.layers) - 10} more layers\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compilation and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Setup callbacks using extracted model\nprint(\"Setting up training callbacks...\")\n\n# Get callbacks from the model\ncallbacks = transfer_model.get_callbacks(log_dir=\"../logs/transfer_learning\")\n\nprint(\"Callbacks configured:\")\nfor callback in callbacks:\n    print(f\"  - {callback.__class__.__name__}\")\n\nprint(\"\\nModel compilation completed using extracted modules.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Training (Frozen Base Model)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Train the model using extracted trainer\nprint(\"Starting training using extracted trainer...\")\n\n# Create classifier and trainer\nclassifier = TransferLearningClassifier(config=config, class_names=class_names)\ntrainer = TransferLearningTrainer(classifier, config)\n\n# Train the model (this handles both phases: frozen base + fine-tuning)\nresults = trainer.train(str(dataset_path))\n\n# Extract results\nmodel = results['model']\ntraining_metrics = results['metrics']\ntraining_history = results['training_history']\nfine_tune_history = results['fine_tune_history']\n\nprint(f\"\\nTraining completed successfully!\")\nprint(f\"Test accuracy: {training_metrics['test_accuracy']:.4f}\")\nprint(f\"Best validation accuracy: {training_metrics['best_val_accuracy']:.4f}\")\nprint(f\"Model parameters: {training_metrics['model_parameters']:,}\")\nprint(f\"Fine-tuned: {training_metrics['fine_tuned']}\")\n\n# Store results for later use\nclassifier.model = model"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Fine-tuning is handled automatically by the trainer\nprint(\"Fine-tuning phase was handled automatically by the trainer.\")\n\nif fine_tune_history:\n    print(\"Fine-tuning was performed successfully.\")\n    print(f\"Phase 1 (frozen base): {len(training_history['loss'])} epochs\")\n    print(f\"Phase 2 (fine-tuning): {len(fine_tune_history['loss'])} epochs\")\nelse:\n    print(\"Fine-tuning was skipped (fine_tune_layers = 0)\")\n\n# Plot training history\nprint(\"\\nPlotting training history...\")\ntry:\n    trainer.plot_training_history(save_path=\"../logs/transfer_learning/training_history.png\")\nexcept Exception as e:\n    print(f\"Could not plot training history: {e}\")\n\nprint(\"Training visualization completed.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Model evaluation using test dataset from trainer results\nprint(\"Evaluating model using test results from trainer...\")\n\ntest_results = results['test_results']\ntest_accuracy = test_results['accuracy']\ntest_loss = test_results['loss']\n\nprint(f\"Test Results:\")\nprint(f\"Loss: {test_loss:.4f}\")\nprint(f\"Accuracy: {test_accuracy:.4f}\")\n\n# Get predictions from trainer results\ny_pred = np.array(test_results['predictions'])\ny_true = np.array(test_results['true_labels'])\n\nprint(f\"\\nEvaluation based on {len(y_true)} test samples\")\n\n# Classification report (show first 10 classes for readability)\nunique_classes = sorted(list(set(y_true)))\ndisplay_classes = unique_classes[:10]\n\nif len(display_classes) < len(unique_classes):\n    print(f\"Note: Showing first 10 of {len(unique_classes)} classes\")\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred, \n                          target_names=[class_names[i] for i in display_classes],\n                          labels=display_classes))\n\n# Confusion matrix (only for manageable number of classes)\nif len(class_names) <= 15:\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.xticks(rotation=45, ha='right')\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(f\"Confusion matrix skipped (too many classes: {len(class_names)})\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Use the TransferLearningClassifier from extracted modules\nprint(\"Transfer learning classifier created using extracted modules.\")\n\n# The classifier is already created and trained\nprint(\"Classifier metadata:\")\nmetadata = classifier.get_metadata()\nfor key, value in metadata.items():\n    print(f\"  {key}: {value}\")\n\nprint(f\"\\nTraining completed on {len(class_names)} classes\")\nprint(f\"Classes: {', '.join(class_names[:10])}{'...' if len(class_names) > 10 else ''}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on validation set\n",
    "val_loss, val_accuracy, val_top_k = model.evaluate(val_dataset, verbose=0)\n",
    "print(f\"Validation Results:\")\n",
    "print(f\"Loss: {val_loss:.4f}\")\n",
    "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Top-k Accuracy: {val_top_k:.4f}\")\n",
    "\n",
    "# Generate predictions for confusion matrix (limited sample)\n",
    "print(\"\\nGenerating predictions for confusion matrix...\")\n",
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "\n",
    "# Collect predictions from validation dataset (limit to first 1000 samples for efficiency)\n",
    "samples_collected = 0\n",
    "max_samples = 1000\n",
    "\n",
    "for images, labels in val_dataset:\n",
    "    if samples_collected >= max_samples:\n",
    "        break\n",
    "    \n",
    "    predictions = model.predict(images, verbose=0)\n",
    "    y_pred_list.extend(np.argmax(predictions, axis=1))\n",
    "    y_true_list.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    samples_collected += len(images)\n",
    "    \n",
    "    if samples_collected % 200 == 0:\n",
    "        print(f\"Processed {samples_collected} samples...\")\n",
    "\n",
    "y_pred = np.array(y_pred_list)\n",
    "y_true = np.array(y_true_list)\n",
    "\n",
    "# Classification report (show first 10 classes for readability)\n",
    "print(f\"\\nClassification Report (based on {len(y_true)} samples, first 10 classes):\")\n",
    "unique_classes = sorted(list(set(y_true)))\n",
    "display_classes = unique_classes[:10]\n",
    "\n",
    "if len(display_classes) < len(unique_classes):\n",
    "    print(f\"Note: Showing first 10 of {len(unique_classes)} classes\")\n",
    "\n",
    "print(classification_report(y_true, y_pred, \n",
    "                          target_names=[class_names[i] for i in display_classes],\n",
    "                          labels=display_classes))\n",
    "\n",
    "# Confusion matrix (only for manageable number of classes)\n",
    "if len(class_names) <= 15:\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Confusion matrix skipped (too many classes: {len(class_names)})\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Save model and register using extracted modules\nprint(\"Saving model using extracted modules...\")\n\n# Save the model\nmodel_save_path = '../models/transfer_learning_classifier.h5'\nclassifier.save_model(\n    model_save_path,\n    model=model,\n    class_names=class_names,\n    accuracy=training_metrics['test_accuracy'],\n    training_history=training_history\n)\n\n# Register model in the model registry\nregistry = ModelRegistry()\n\n# Create metadata using training results\nmetadata = ModelMetadata(\n    name=\"transfer_learning_classifier\",\n    version=\"1.0.0\",\n    model_type=\"transfer_learning\",\n    accuracy=training_metrics['test_accuracy'],\n    training_date=datetime.now().isoformat(),\n    model_path=model_save_path,\n    config=config.to_dict(),\n    performance_metrics=training_metrics\n)\n\n# Register the model\nregistry.register_model(metadata)\nprint(\"Model registered successfully in the model registry.\")\n\n# Save training history\nhistory_path = '../logs/transfer_learning_training_history.json'\ntrainer.save_training_history(history_path)\nprint(f\"Training history saved to {history_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Transfer Learning Development Summary ===\")\nprint(\"✅ Successfully extracted transfer learning code into modular src files\")\nprint(\"✅ Updated notebook to use extracted modules\")\nprint()\nprint(f\"Base Model: {config.base_model_name}\")\nprint(f\"Framework: TensorFlow/Keras\")\nprint(f\"Training Strategy: Two-phase (frozen + fine-tuning)\")\nprint(f\"Final Test Accuracy: {training_metrics['test_accuracy']:.4f}\")\nprint(f\"Best Validation Accuracy: {training_metrics['best_val_accuracy']:.4f}\")\nprint(f\"Model Parameters: {training_metrics['model_parameters']:,}\")\nprint(f\"Total Classes: {len(class_names)}\")\nprint(f\"Training Samples: {training_metrics['train_samples']}\")\nprint(f\"Test Samples: {training_metrics['test_samples']}\")\n\nprint(f\"\\nExtracted Modules:\")\nprint(\"- src/config.py: TransferLearningClassifierConfig\")\nprint(\"- src/models.py: TransferLearningModel with TensorFlow/Keras\")\nprint(\"- src/data_loader.py: Memory-efficient TensorFlow datasets\")\nprint(\"- src/trainer.py: TransferLearningTrainer with two-phase training\")\nprint(\"- src/classifier.py: TransferLearningClassifier implementing BaseImageClassifier\")\nprint(\"- scripts/train.py: CLI training script\")\n\nprint(f\"\\nKey Features Implemented:\")\nprint(\"✅ Pre-trained ImageNet weights\")\nprint(\"✅ Two-phase training (frozen base + fine-tuning)\")\nprint(\"✅ Memory-efficient tf.data loading\")\nprint(\"✅ Mixed precision training\")\nprint(\"✅ XLA compilation\")\nprint(\"✅ Class weight balancing\")\nprint(\"✅ Comprehensive callbacks\")\nprint(\"✅ TensorBoard logging\")\nprint(\"✅ Model checkpointing\")\n\nprint(f\"\\nIntegration Features:\")\nprint(\"✅ Implements BaseImageClassifier interface\")\nprint(\"✅ Compatible with ModelRegistry\")\nprint(\"✅ Configurable via dataclass\")\nprint(\"✅ Memory-efficient data loading\")\nprint(\"✅ Production-ready CLI script\")\n\nprint(f\"\\nNext Steps:\")\nprint(\"1. ✅ Code extraction completed\")\nprint(\"2. ✅ Notebook integration completed\")\nprint(\"3. Run CLI training script for validation\")\nprint(\"4. Add unit tests for extracted modules\")\nprint(\"5. Deploy to production API\")\nprint(\"6. Monitor model performance\")\n\nprint(f\"\\nCLI Usage:\")\nprint(\"python scripts/train.py --data_path /path/to/data --base_model resnet50 --epochs 20\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}