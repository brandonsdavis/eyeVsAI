{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Image Classifier Development\n",
    "\n",
    "This notebook demonstrates the development of an image classifier using transfer learning with pre-trained models. We'll leverage established architectures trained on ImageNet and fine-tune them for our specific classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50, VGG16, EfficientNetB0\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport os\nimport json\nfrom datetime import datetime\nimport sys\nsys.path.append('../..')\nfrom ml_models_core.src.base_classifier import BaseImageClassifier\nfrom ml_models_core.src.model_registry import ModelRegistry, ModelMetadata\nfrom ml_models_core.src.data_loaders import get_unified_classification_data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def load_and_preprocess_data():\n    \"\"\"\n    Load and preprocess the unified classification dataset for transfer learning.\n    \"\"\"\n    print(\"Loading unified classification dataset...\")\n    \n    # Load the unified dataset (use sklearn format for raw data access)\n    data_split = get_unified_classification_data(framework='sklearn')\n    \n    # Extract components from DataSplit object\n    X = np.vstack([data_split.X_train, data_split.X_val, data_split.X_test])\n    y = np.hstack([data_split.y_train, data_split.y_val, data_split.y_test])\n    class_names = data_split.class_names\n    \n    print(f\"Loaded {len(X)} images from {len(class_names)} classes\")\n    print(f\"Classes (first 10): {class_names[:10]}\")\n    print(f\"Total classes: {len(class_names)}\")\n    print(f\"Image shape: {X[0].shape}\")\n    \n    # Update CONFIG with actual number of classes\n    CONFIG['num_classes'] = len(class_names)\n    \n    # Convert to numpy arrays if needed\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    if not isinstance(y, np.ndarray):\n        y = np.array(y)\n    \n    # Resize images to transfer learning input size (224x224)\n    print(\"Resizing images to 224x224 for transfer learning...\")\n    X_resized = tf.image.resize(X, [224, 224]).numpy()\n    \n    # Normalize pixel values to [0, 1]\n    X_resized = X_resized.astype(np.float32) / 255.0\n    \n    # Convert labels to categorical\n    y_categorical = to_categorical(y, CONFIG['num_classes'])\n    \n    # Split data\n    split_idx = int(len(X_resized) * (1 - CONFIG['validation_split']))\n    \n    X_train, X_val = X_resized[:split_idx], X_resized[split_idx:]\n    y_train, y_val = y_categorical[:split_idx], y_categorical[split_idx:]\n    \n    print(f\"Training samples: {len(X_train)}\")\n    print(f\"Validation samples: {len(X_val)}\")\n    print(f\"Class distribution in training: {np.bincount(np.argmax(y_train, axis=1))}\")\n    \n    return X_train, X_val, y_train, y_val, class_names\n\n# Load the data\nX_train, X_val, y_train, y_val, class_names = load_and_preprocess_data()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def load_and_preprocess_data():\n    \"\"\"\n    Load and preprocess the unified classification dataset for transfer learning.\n    \"\"\"\n    print(\"Loading unified classification dataset...\")\n    \n    # Load the unified dataset\n    data = get_unified_classification_data(framework='tensorflow')\n    \n    # Extract components\n    X = data['X']\n    y = data['y']\n    class_names = data['class_names']\n    \n    print(f\"Loaded {len(X)} images from {len(class_names)} classes\")\n    print(f\"Classes (first 10): {class_names[:10]}\")\n    print(f\"Total classes: {len(class_names)}\")\n    print(f\"Image shape: {X[0].shape}\")\n    \n    # Update CONFIG with actual number of classes\n    CONFIG['num_classes'] = len(class_names)\n    \n    # Convert to numpy arrays if needed\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n    if not isinstance(y, np.ndarray):\n        y = np.array(y)\n    \n    # Resize images to transfer learning input size (224x224)\n    print(\"Resizing images to 224x224 for transfer learning...\")\n    X_resized = tf.image.resize(X, [224, 224]).numpy()\n    \n    # Normalize pixel values to [0, 1]\n    X_resized = X_resized.astype(np.float32) / 255.0\n    \n    # Convert labels to categorical\n    y_categorical = to_categorical(y, CONFIG['num_classes'])\n    \n    # Split data\n    split_idx = int(len(X_resized) * (1 - CONFIG['validation_split']))\n    \n    X_train, X_val = X_resized[:split_idx], X_resized[split_idx:]\n    y_train, y_val = y_categorical[:split_idx], y_categorical[split_idx:]\n    \n    print(f\"Training samples: {len(X_train)}\")\n    print(f\"Validation samples: {len(X_val)}\")\n    print(f\"Class distribution in training: {np.bincount(np.argmax(y_train, axis=1))}\")\n    \n    return X_train, X_val, y_train, y_val, class_names\n\n# Load the data\nX_train, X_val, y_train, y_val, class_names = load_and_preprocess_data()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Model Selection and Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_model(model_name='resnet50', input_shape=(224, 224, 3)):\n",
    "    \"\"\"\n",
    "    Create and return a pre-trained base model.\n",
    "    \"\"\"\n",
    "    base_models = {\n",
    "        'resnet50': ResNet50,\n",
    "        'vgg16': VGG16,\n",
    "        'efficientnet': EfficientNetB0\n",
    "    }\n",
    "    \n",
    "    if model_name not in base_models:\n",
    "        raise ValueError(f\"Model {model_name} not supported. Choose from: {list(base_models.keys())}\")\n",
    "    \n",
    "    base_model = base_models[model_name](\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Freeze base model layers initially\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    print(f\"Base model: {model_name}\")\n",
    "    print(f\"Total parameters: {base_model.count_params():,}\")\n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in base_model.trainable_weights if p.trainable):,}\")\n",
    "    \n",
    "    return base_model\n",
    "\n",
    "def create_transfer_model(base_model, num_classes=2, dropout_rate=0.5):\n",
    "    \"\"\"\n",
    "    Create the complete transfer learning model with custom head.\n",
    "    \"\"\"\n",
    "    # Add custom classification head\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(CONFIG['l2_regularization']))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(CONFIG['l2_regularization']))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the transfer learning model\n",
    "base_model = create_base_model(CONFIG['base_model'], CONFIG['input_shape'])\n",
    "model = create_transfer_model(base_model, CONFIG['num_classes'], CONFIG['dropout_rate'])\n",
    "\n",
    "print(f\"\\nComplete model summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compilation and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=CONFIG['learning_rate']),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'top_k_categorical_accuracy']\n",
    ")\n",
    "\n",
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        '../models/transfer_model_best.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Model compiled with callbacks ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Training (Frozen Base Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting initial training with frozen base model...\")\n",
    "\n",
    "# Train with frozen base model\n",
    "history_initial = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    epochs=CONFIG['epochs'],\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Initial training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning (Unfreezing Base Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting fine-tuning phase...\")\n",
    "\n",
    "# Unfreeze the base model for fine-tuning\n",
    "base_model.trainable = True\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = len(base_model.layers) // 2\n",
    "\n",
    "# Freeze all layers before fine_tune_at\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(f\"Fine-tuning from layer {fine_tune_at} onwards\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.trainable_weights):,}\")\n",
    "\n",
    "# Recompile with lower learning rate for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=CONFIG['fine_tune_learning_rate']),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'top_k_categorical_accuracy']\n",
    ")\n",
    "\n",
    "# Continue training with fine-tuning\n",
    "history_finetune = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    epochs=CONFIG['fine_tune_epochs'],\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history_initial, history_finetune):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics for both training phases.\n",
    "    \"\"\"\n",
    "    # Combine histories\n",
    "    initial_epochs = len(history_initial.history['loss'])\n",
    "    total_epochs = initial_epochs + len(history_finetune.history['loss'])\n",
    "    \n",
    "    # Combine metrics\n",
    "    train_loss = history_initial.history['loss'] + history_finetune.history['loss']\n",
    "    val_loss = history_initial.history['val_loss'] + history_finetune.history['val_loss']\n",
    "    train_acc = history_initial.history['accuracy'] + history_finetune.history['accuracy']\n",
    "    val_acc = history_initial.history['val_accuracy'] + history_finetune.history['val_accuracy']\n",
    "    \n",
    "    epochs = range(1, total_epochs + 1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
    "    ax1.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
    "    ax1.axvline(x=initial_epochs, color='g', linestyle='--', alpha=0.7, label='Fine-tuning Start')\n",
    "    ax1.set_title('Model Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(epochs, train_acc, 'b-', label='Training Accuracy')\n",
    "    ax2.plot(epochs, val_acc, 'r-', label='Validation Accuracy')\n",
    "    ax2.axvline(x=initial_epochs, color='g', linestyle='--', alpha=0.7, label='Fine-tuning Start')\n",
    "    ax2.set_title('Model Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Best validation accuracy: {max(val_acc):.4f}\")\n",
    "    print(f\"Final validation accuracy: {val_acc[-1]:.4f}\")\n",
    "\n",
    "plot_training_history(history_initial, history_finetune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate model on validation set\nval_loss, val_accuracy, val_top_k = model.evaluate(X_val, y_val, verbose=0)\nprint(f\"Validation Results:\")\nprint(f\"Loss: {val_loss:.4f}\")\nprint(f\"Accuracy: {val_accuracy:.4f}\")\nprint(f\"Top-k Accuracy: {val_top_k:.4f}\")\n\n# Generate predictions\ny_pred_probs = model.predict(X_val)\ny_pred = np.argmax(y_pred_probs, axis=1)\ny_true = np.argmax(y_val, axis=1)\n\n# Classification report (show first 10 classes for readability)\nprint(\"\\nClassification Report (first 10 classes):\")\nunique_classes = sorted(list(set(y_true)))\ndisplay_classes = unique_classes[:10]\n\nif len(display_classes) < len(unique_classes):\n    print(f\"Note: Showing first 10 of {len(unique_classes)} classes\")\n\nprint(classification_report(y_true, y_pred, \n                          target_names=[class_names[i] for i in display_classes],\n                          labels=display_classes))\n\n# Confusion matrix (only for manageable number of classes)\nif len(class_names) <= 15:\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.xticks(rotation=45, ha='right')\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(f\"Confusion matrix skipped (too many classes: {len(class_names)})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Classifier Implementation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "class TransferLearningClassifier(BaseImageClassifier):\n    \"\"\"\n    Transfer learning image classifier using pre-trained models.\n    \"\"\"\n    \n    def __init__(self, config=None, class_names=None):\n        self.config = config or CONFIG\n        self.model = None\n        self.class_names = class_names or ['Class_0', 'Class_1']\n        self.training_history = None\n        \n    def load_model(self, model_path: str) -> None:\n        \"\"\"\n        Load a trained transfer learning model.\n        \"\"\"\n        try:\n            self.model = tf.keras.models.load_model(model_path)\n            print(f\"Model loaded from {model_path}\")\n        except Exception as e:\n            print(f\"Error loading model: {e}\")\n            raise\n    \n    def preprocess(self, image: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Preprocess image for transfer learning model.\n        \"\"\"\n        # Convert to float32 and normalize\n        if image.dtype != np.float32:\n            image = image.astype(np.float32)\n        \n        # Resize to model input size\n        if image.shape[:2] != self.config['input_shape'][:2]:\n            image = tf.image.resize(image, self.config['input_shape'][:2])\n        \n        # Normalize pixel values to [0, 1] if not already normalized\n        if image.max() > 1.0:\n            image = image / 255.0\n        \n        # Add batch dimension if needed\n        if len(image.shape) == 3:\n            image = np.expand_dims(image, axis=0)\n        \n        return image\n    \n    def predict(self, image: np.ndarray) -> dict:\n        \"\"\"\n        Make prediction on preprocessed image.\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"Model not loaded. Call load_model() first.\")\n        \n        preprocessed_image = self.preprocess(image)\n        predictions = self.model.predict(preprocessed_image, verbose=0)\n        \n        # Convert to probabilities dict\n        probs = predictions[0] if len(predictions.shape) > 1 else predictions\n        \n        return {\n            self.class_names[i]: float(prob) \n            for i, prob in enumerate(probs)\n        }\n    \n    def get_metadata(self) -> dict:\n        \"\"\"\n        Get model metadata and configuration.\n        \"\"\"\n        return {\n            'model_type': 'transfer_learning',\n            'base_model': self.config['base_model'],\n            'input_shape': self.config['input_shape'],\n            'num_classes': self.config['num_classes'],\n            'class_names': self.class_names,\n            'preprocessing': 'resize_and_normalize',\n            'framework': 'tensorflow',\n            'architecture': 'pretrained_with_custom_head'\n        }\n    \n    def save_model(self, model_path: str) -> None:\n        \"\"\"\n        Save the trained model.\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"No model to save. Train or load a model first.\")\n        \n        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n        self.model.save(model_path)\n        print(f\"Model saved to {model_path}\")\n\n# Create classifier instance with actual class names\ntransfer_classifier = TransferLearningClassifier(CONFIG, class_names)\ntransfer_classifier.model = model\n\nprint(\"Transfer learning classifier created successfully.\")\nprint(\"Metadata:\", transfer_classifier.get_metadata())\nprint(f\"Training on {len(class_names)} classes: {class_names[:10]}{'...' if len(class_names) > 10 else ''}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_performance(classifier, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Comprehensive performance analysis of the transfer learning model.\n",
    "    \"\"\"\n",
    "    print(\"=== Transfer Learning Model Performance Analysis ===\")\n",
    "    \n",
    "    # Test predictions\n",
    "    predictions = []\n",
    "    for i, image in enumerate(X_test[:100]):  # Test on subset for demonstration\n",
    "        pred = classifier.predict(image)\n",
    "        predictions.append(pred)\n",
    "        if i % 20 == 0:\n",
    "            print(f\"Processed {i+1}/100 test images\")\n",
    "    \n",
    "    # Convert predictions to arrays\n",
    "    pred_probs = np.array([[pred[class_name] for class_name in classifier.class_names] \n",
    "                          for pred in predictions])\n",
    "    pred_classes = np.argmax(pred_probs, axis=1)\n",
    "    true_classes = np.argmax(y_test[:100], axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(pred_classes == true_classes)\n",
    "    \n",
    "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Average Confidence: {np.mean(np.max(pred_probs, axis=1)):.4f}\")\n",
    "    \n",
    "    # Plot prediction confidence distribution\n",
    "    confidences = np.max(pred_probs, axis=1)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(confidences, bins=20, alpha=0.7, edgecolor='black')\n",
    "    plt.title('Prediction Confidence Distribution')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.axvline(np.mean(confidences), color='red', linestyle='--', \n",
    "               label=f'Mean: {np.mean(confidences):.3f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot accuracy vs confidence\n",
    "    plt.subplot(1, 2, 2)\n",
    "    correct = (pred_classes == true_classes)\n",
    "    plt.scatter(confidences[correct], [1]*sum(correct), alpha=0.6, \n",
    "               label='Correct', color='green')\n",
    "    plt.scatter(confidences[~correct], [0]*sum(~correct), alpha=0.6, \n",
    "               label='Incorrect', color='red')\n",
    "    plt.title('Prediction Accuracy vs Confidence')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Correct (1) / Incorrect (0)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'mean_confidence': np.mean(confidences),\n",
    "        'std_confidence': np.std(confidences)\n",
    "    }\n",
    "\n",
    "# Analyze performance\n",
    "performance_metrics = analyze_model_performance(transfer_classifier, X_val, y_val)\n",
    "print(f\"\\nFinal Performance Summary:\")\n",
    "for metric, value in performance_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Registry Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register model in the model registry\n",
    "registry = ModelRegistry()\n",
    "\n",
    "# Save the model\n",
    "model_save_path = '../models/transfer_learning_classifier.h5'\n",
    "transfer_classifier.save_model(model_save_path)\n",
    "\n",
    "# Create metadata\n",
    "metadata = ModelMetadata(\n",
    "    name=\"transfer_learning_classifier\",\n",
    "    version=\"1.0.0\",\n",
    "    model_type=\"transfer_learning\",\n",
    "    accuracy=performance_metrics['accuracy'],\n",
    "    training_date=datetime.now().isoformat(),\n",
    "    model_path=model_save_path,\n",
    "    config=CONFIG,\n",
    "    performance_metrics={\n",
    "        'validation_accuracy': val_accuracy,\n",
    "        'validation_loss': val_loss,\n",
    "        'mean_confidence': performance_metrics['mean_confidence'],\n",
    "        'std_confidence': performance_metrics['std_confidence']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Register the model\n",
    "registry.register_model(metadata)\n",
    "print(\"Model registered successfully in the model registry.\")\n",
    "\n",
    "# Save configuration\n",
    "config_path = '../models/transfer_learning_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "print(f\"Configuration saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Transfer Learning Development Summary ===\")\nprint(f\"Base Model: {CONFIG['base_model']}\")\nprint(f\"Training Strategy: Two-phase (frozen + fine-tuning)\")\nprint(f\"Final Validation Accuracy: {val_accuracy:.4f}\")\nprint(f\"Model Parameters: {model.count_params():,}\")\nprint(f\"Total Classes: {len(class_names)}\")\nprint(f\"Training Images: {len(X_train)}\")\nprint(f\"Validation Images: {len(X_val)}\")\n\nprint(f\"\\nDataset Information:\")\nprint(f\"- Unified classification dataset with {len(class_names)} classes\")\nprint(f\"- Classes include: {', '.join(class_names[:5])}{'...' if len(class_names) > 5 else ''}\")\nprint(f\"- Images resized to {CONFIG['input_shape'][:2]} for transfer learning\")\n\nprint(f\"\\nKey Features:\")\nprint(\"- Pre-trained ImageNet weights (ResNet50)\")\nprint(\"- Custom classification head\")\nprint(\"- Two-phase training strategy\")\nprint(\"- Comprehensive evaluation metrics\")\nprint(\"- Compatible with unified dataset\")\n\nprint(f\"\\nModel Integration:\")\nprint(\"- Implements BaseImageClassifier interface\")\nprint(\"- Registered in ModelRegistry\")\nprint(\"- Ready for ensemble integration\")\nprint(\"- Compatible with API deployment\")\n\nprint(f\"\\nNext Steps:\")\nprint(\"1. Experiment with different pre-trained models\")\nprint(\"2. Optimize hyperparameters\")\nprint(\"3. Implement model ensembling\")\nprint(\"4. Deploy to production API\")\nprint(\"5. Monitor model performance\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}