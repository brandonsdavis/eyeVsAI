{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Image Classifier Development\n",
    "\n",
    "This notebook demonstrates the development of an image classifier using transfer learning with pre-trained models. We'll leverage established architectures trained on ImageNet and fine-tune them for our specific classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50, VGG16, EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from ml_models_core.src.base_classifier import BaseImageClassifier\n",
    "from ml_models_core.src.model_registry import ModelRegistry, ModelMetadata\n",
    "from ml_models_core.src.data_loaders import get_unified_classification_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-Efficient Data Loading\n",
    "\n",
    "**Note**: This notebook uses memory-efficient data loading similar to the deep learning notebooks. Instead of loading all images into memory at once, we:\n",
    "1. Store only file paths in memory\n",
    "2. Load images on-demand during training using TensorFlow's tf.data API\n",
    "3. Use data streaming and prefetching for optimal performance\n",
    "\n",
    "This approach allows training on large datasets without running out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'base_model': 'resnet50',\n",
    "    'input_shape': (224, 224, 3),\n",
    "    'num_classes': 2,  # Will be updated based on dataset\n",
    "    'batch_size': 32,\n",
    "    'epochs': 20,\n",
    "    'fine_tune_epochs': 10,\n",
    "    'learning_rate': 1e-3,\n",
    "    'fine_tune_learning_rate': 1e-5,\n",
    "    'dropout_rate': 0.5,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'validation_split': 0.2\n",
    "}\n",
    "\n",
    "def create_memory_efficient_dataset():\n",
    "    \"\"\"\n",
    "    Create memory-efficient TensorFlow datasets that load images on-demand.\n",
    "    \"\"\"\n",
    "    print(\"Creating memory-efficient dataset...\")\n",
    "    \n",
    "    # Use the existing combined dataset\n",
    "    from pathlib import Path\n",
    "    dataset_path = Path(\"../../data/downloads/combined_unified_classification\")\n",
    "    \n",
    "    if not dataset_path.exists():\n",
    "        # Fallback to other available datasets\n",
    "        base_data_dir = Path(\"../../data/downloads\")\n",
    "        available_datasets = [\n",
    "            base_data_dir / \"combined_unified_classification\",\n",
    "            base_data_dir / \"oxford_pets\",\n",
    "            base_data_dir / \"vegetables\"\n",
    "        ]\n",
    "        \n",
    "        for candidate in available_datasets:\n",
    "            if candidate.exists():\n",
    "                dataset_path = candidate\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No datasets found. Please run data preparation first.\")\n",
    "    \n",
    "    print(f\"Dataset path: {dataset_path}\")\n",
    "    \n",
    "    # Collect image paths and labels without loading images\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    class_names = []\n",
    "    class_to_idx = {}\n",
    "    \n",
    "    # Scan directory structure\n",
    "    for class_idx, class_dir in enumerate(sorted(dataset_path.iterdir())):\n",
    "        if not class_dir.is_dir() or class_dir.name.startswith('.'):\n",
    "            continue\n",
    "        \n",
    "        class_name = class_dir.name\n",
    "        class_names.append(class_name)\n",
    "        class_to_idx[class_name] = class_idx\n",
    "        \n",
    "        # Collect paths for this class\n",
    "        valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\n",
    "        for ext in valid_extensions:\n",
    "            for img_path in class_dir.glob(f'*{ext}'):\n",
    "                image_paths.append(str(img_path))\n",
    "                labels.append(class_idx)\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} images from {len(class_names)} classes\")\n",
    "    print(f\"Classes (first 10): {class_names[:10]}\")\n",
    "    \n",
    "    # Update CONFIG with actual number of classes\n",
    "    CONFIG['num_classes'] = len(class_names)\n",
    "    \n",
    "    # Convert to numpy arrays for splitting\n",
    "    image_paths = np.array(image_paths)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Shuffle data\n",
    "    indices = np.random.permutation(len(image_paths))\n",
    "    image_paths = image_paths[indices]\n",
    "    labels = labels[indices]\n",
    "    \n",
    "    # Split into train/val\n",
    "    split_idx = int(len(image_paths) * (1 - CONFIG['validation_split']))\n",
    "    train_paths = image_paths[:split_idx]\n",
    "    train_labels = labels[:split_idx]\n",
    "    val_paths = image_paths[split_idx:]\n",
    "    val_labels = labels[split_idx:]\n",
    "    \n",
    "    print(f\"Training samples: {len(train_paths)}\")\n",
    "    print(f\"Validation samples: {len(val_paths)}\")\n",
    "    \n",
    "    # Create TensorFlow datasets with memory-efficient loading\n",
    "    def load_and_preprocess_image(path, label):\n",
    "        \"\"\"Load and preprocess a single image.\"\"\"\n",
    "        # Load image from file\n",
    "        image = tf.io.read_file(path)\n",
    "        image = tf.image.decode_image(image, channels=3, expand_animations=False)\n",
    "        image = tf.ensure_shape(image, [None, None, 3])\n",
    "        # Resize to model input size\n",
    "        image = tf.image.resize(image, [224, 224])\n",
    "        # Normalize to [0, 1]\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "        # Convert label to one-hot\n",
    "        label = tf.one_hot(label, CONFIG['num_classes'])\n",
    "        return image, label\n",
    "    \n",
    "    # Create datasets from paths\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "    train_dataset = train_dataset.map(load_and_preprocess_image, \n",
    "                                      num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_dataset = train_dataset.batch(CONFIG['batch_size'])\n",
    "    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "    val_dataset = val_dataset.map(load_and_preprocess_image,\n",
    "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(CONFIG['batch_size'])\n",
    "    val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Create augmented training dataset\n",
    "    def augment_image(image, label):\n",
    "        \"\"\"Apply data augmentation to training images.\"\"\"\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_brightness(image, 0.1)\n",
    "        image = tf.image.random_contrast(image, 0.9, 1.1)\n",
    "        return image, label\n",
    "    \n",
    "    train_dataset_aug = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "    train_dataset_aug = train_dataset_aug.map(load_and_preprocess_image,\n",
    "                                              num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_dataset_aug = train_dataset_aug.map(augment_image,\n",
    "                                              num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_dataset_aug = train_dataset_aug.batch(CONFIG['batch_size'])\n",
    "    train_dataset_aug = train_dataset_aug.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_dataset_aug, val_dataset, class_names, len(train_paths), len(val_paths)\n",
    "\n",
    "# Create memory-efficient datasets\n",
    "train_dataset, val_dataset, class_names, n_train, n_val = create_memory_efficient_dataset()\n",
    "\n",
    "print(f\"\\nMemory-efficient datasets created successfully!\")\n",
    "print(f\"Training on {CONFIG['num_classes']} classes\")\n",
    "print(f\"Training batches: {n_train // CONFIG['batch_size']}\")\n",
    "print(f\"Validation batches: {n_val // CONFIG['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Model Selection and Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_model(model_name='resnet50', input_shape=(224, 224, 3)):\n",
    "    \"\"\"\n",
    "    Create and return a pre-trained base model.\n",
    "    \"\"\"\n",
    "    base_models = {\n",
    "        'resnet50': ResNet50,\n",
    "        'vgg16': VGG16,\n",
    "        'efficientnet': EfficientNetB0\n",
    "    }\n",
    "    \n",
    "    if model_name not in base_models:\n",
    "        raise ValueError(f\"Model {model_name} not supported. Choose from: {list(base_models.keys())}\")\n",
    "    \n",
    "    base_model = base_models[model_name](\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Freeze base model layers initially\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    print(f\"Base model: {model_name}\")\n",
    "    print(f\"Total parameters: {base_model.count_params():,}\")\n",
    "    print(f\"Trainable parameters: {sum(tf.size(w).numpy() for w in base_model.trainable_weights):,}\")\n",
    "    \n",
    "    return base_model\n",
    "\n",
    "def create_transfer_model(base_model, num_classes=2, dropout_rate=0.5):\n",
    "    \"\"\"\n",
    "    Create the complete transfer learning model with custom head.\n",
    "    \"\"\"\n",
    "    # Add custom classification head\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(CONFIG['l2_regularization']))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(CONFIG['l2_regularization']))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the transfer learning model\n",
    "base_model = create_base_model(CONFIG['base_model'], CONFIG['input_shape'])\n",
    "model = create_transfer_model(base_model, CONFIG['num_classes'], CONFIG['dropout_rate'])\n",
    "\n",
    "print(f\"\\nComplete model summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compilation and Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=CONFIG['learning_rate']),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'top_k_categorical_accuracy']\n",
    ")\n",
    "\n",
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        '../models/transfer_model_best.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Model compiled with callbacks ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Training (Frozen Base Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting initial training with frozen base model...\")\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = n_train // CONFIG['batch_size']\n",
    "validation_steps = n_val // CONFIG['batch_size']\n",
    "\n",
    "# Train with frozen base model\n",
    "history_initial = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    validation_data=val_dataset,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Initial training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting fine-tuning phase...\")\n",
    "\n",
    "# Unfreeze the base model for fine-tuning\n",
    "base_model.trainable = True\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = len(base_model.layers) // 2\n",
    "\n",
    "# Freeze all layers before fine_tune_at\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(f\"Fine-tuning from layer {fine_tune_at} onwards\")\n",
    "print(f\"Trainable parameters: {sum(tf.size(w).numpy() for w in model.trainable_weights):,}\")\n",
    "\n",
    "# Recompile with lower learning rate for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=CONFIG['fine_tune_learning_rate']),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'top_k_categorical_accuracy']\n",
    ")\n",
    "\n",
    "# Continue training with fine-tuning\n",
    "history_finetune = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=CONFIG['fine_tune_epochs'],\n",
    "    validation_data=val_dataset,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on validation set\n",
    "val_loss, val_accuracy, val_top_k = model.evaluate(val_dataset, verbose=0)\n",
    "print(f\"Validation Results:\")\n",
    "print(f\"Loss: {val_loss:.4f}\")\n",
    "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Top-k Accuracy: {val_top_k:.4f}\")\n",
    "\n",
    "# Generate predictions for confusion matrix (limited sample)\n",
    "print(\"\\nGenerating predictions for confusion matrix...\")\n",
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "\n",
    "# Collect predictions from validation dataset (limit to first 1000 samples for efficiency)\n",
    "samples_collected = 0\n",
    "max_samples = 1000\n",
    "\n",
    "for images, labels in val_dataset:\n",
    "    if samples_collected >= max_samples:\n",
    "        break\n",
    "    \n",
    "    predictions = model.predict(images, verbose=0)\n",
    "    y_pred_list.extend(np.argmax(predictions, axis=1))\n",
    "    y_true_list.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    samples_collected += len(images)\n",
    "    \n",
    "    if samples_collected % 200 == 0:\n",
    "        print(f\"Processed {samples_collected} samples...\")\n",
    "\n",
    "y_pred = np.array(y_pred_list)\n",
    "y_true = np.array(y_true_list)\n",
    "\n",
    "# Classification report (show first 10 classes for readability)\n",
    "print(f\"\\nClassification Report (based on {len(y_true)} samples, first 10 classes):\")\n",
    "unique_classes = sorted(list(set(y_true)))\n",
    "display_classes = unique_classes[:10]\n",
    "\n",
    "if len(display_classes) < len(unique_classes):\n",
    "    print(f\"Note: Showing first 10 of {len(unique_classes)} classes\")\n",
    "\n",
    "print(classification_report(y_true, y_pred, \n",
    "                          target_names=[class_names[i] for i in display_classes],\n",
    "                          labels=display_classes))\n",
    "\n",
    "# Confusion matrix (only for manageable number of classes)\n",
    "if len(class_names) <= 15:\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Confusion matrix skipped (too many classes: {len(class_names)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferLearningClassifier(BaseImageClassifier):\n",
    "    \"\"\"\n",
    "    Transfer learning image classifier using pre-trained models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config=None, class_names=None):\n",
    "        self.config = config or CONFIG\n",
    "        self.model = None\n",
    "        self.class_names = class_names or ['Class_0', 'Class_1']\n",
    "        self.training_history = None\n",
    "        \n",
    "    def load_model(self, model_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Load a trained transfer learning model.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = tf.keras.models.load_model(model_path)\n",
    "            print(f\"Model loaded from {model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def preprocess(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Preprocess image for transfer learning model.\n",
    "        \"\"\"\n",
    "        # Convert to float32 and normalize\n",
    "        if image.dtype != np.float32:\n",
    "            image = image.astype(np.float32)\n",
    "        \n",
    "        # Resize to model input size\n",
    "        if image.shape[:2] != self.config['input_shape'][:2]:\n",
    "            image = tf.image.resize(image, self.config['input_shape'][:2])\n",
    "        \n",
    "        # Normalize pixel values to [0, 1] if not already normalized\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        # Add batch dimension if needed\n",
    "        if len(image.shape) == 3:\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def predict(self, image: np.ndarray) -> dict:\n",
    "        \"\"\"\n",
    "        Make prediction on preprocessed image.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not loaded. Call load_model() first.\")\n",
    "        \n",
    "        preprocessed_image = self.preprocess(image)\n",
    "        predictions = self.model.predict(preprocessed_image, verbose=0)\n",
    "        \n",
    "        # Convert to probabilities dict\n",
    "        probs = predictions[0] if len(predictions.shape) > 1 else predictions\n",
    "        \n",
    "        return {\n",
    "            self.class_names[i]: float(prob) \n",
    "            for i, prob in enumerate(probs)\n",
    "        }\n",
    "    \n",
    "    def get_metadata(self) -> dict:\n",
    "        \"\"\"\n",
    "        Get model metadata and configuration.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'model_type': 'transfer_learning',\n",
    "            'base_model': self.config['base_model'],\n",
    "            'input_shape': self.config['input_shape'],\n",
    "            'num_classes': self.config['num_classes'],\n",
    "            'class_names': self.class_names,\n",
    "            'preprocessing': 'resize_and_normalize',\n",
    "            'framework': 'tensorflow',\n",
    "            'architecture': 'pretrained_with_custom_head'\n",
    "        }\n",
    "    \n",
    "    def save_model(self, model_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the trained model.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No model to save. Train or load a model first.\")\n",
    "        \n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        self.model.save(model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Create classifier instance with actual class names\n",
    "transfer_classifier = TransferLearningClassifier(CONFIG, class_names)\n",
    "transfer_classifier.model = model\n",
    "\n",
    "print(\"Transfer learning classifier created successfully.\")\n",
    "print(\"Metadata:\", transfer_classifier.get_metadata())\n",
    "print(f\"Training on {len(class_names)} classes: {class_names[:10]}{'...' if len(class_names) > 10 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on validation set\n",
    "val_loss, val_accuracy, val_top_k = model.evaluate(val_dataset, verbose=0)\n",
    "print(f\"Validation Results:\")\n",
    "print(f\"Loss: {val_loss:.4f}\")\n",
    "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Top-k Accuracy: {val_top_k:.4f}\")\n",
    "\n",
    "# Generate predictions for confusion matrix (limited sample)\n",
    "print(\"\\nGenerating predictions for confusion matrix...\")\n",
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "\n",
    "# Collect predictions from validation dataset (limit to first 1000 samples for efficiency)\n",
    "samples_collected = 0\n",
    "max_samples = 1000\n",
    "\n",
    "for images, labels in val_dataset:\n",
    "    if samples_collected >= max_samples:\n",
    "        break\n",
    "    \n",
    "    predictions = model.predict(images, verbose=0)\n",
    "    y_pred_list.extend(np.argmax(predictions, axis=1))\n",
    "    y_true_list.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    samples_collected += len(images)\n",
    "    \n",
    "    if samples_collected % 200 == 0:\n",
    "        print(f\"Processed {samples_collected} samples...\")\n",
    "\n",
    "y_pred = np.array(y_pred_list)\n",
    "y_true = np.array(y_true_list)\n",
    "\n",
    "# Classification report (show first 10 classes for readability)\n",
    "print(f\"\\nClassification Report (based on {len(y_true)} samples, first 10 classes):\")\n",
    "unique_classes = sorted(list(set(y_true)))\n",
    "display_classes = unique_classes[:10]\n",
    "\n",
    "if len(display_classes) < len(unique_classes):\n",
    "    print(f\"Note: Showing first 10 of {len(unique_classes)} classes\")\n",
    "\n",
    "print(classification_report(y_true, y_pred, \n",
    "                          target_names=[class_names[i] for i in display_classes],\n",
    "                          labels=display_classes))\n",
    "\n",
    "# Confusion matrix (only for manageable number of classes)\n",
    "if len(class_names) <= 15:\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Confusion matrix skipped (too many classes: {len(class_names)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple performance metrics for model registry\n",
    "performance_metrics = {\n",
    "    'accuracy': val_accuracy,\n",
    "    'mean_confidence': 0.75,  # Placeholder - would be calculated from detailed analysis\n",
    "    'std_confidence': 0.15    # Placeholder - would be calculated from detailed analysis\n",
    "}\n",
    "\n",
    "# Register model in the model registry\n",
    "registry = ModelRegistry()\n",
    "\n",
    "# Save the model\n",
    "model_save_path = '../models/transfer_learning_classifier.h5'\n",
    "transfer_classifier.save_model(model_save_path)\n",
    "\n",
    "# Create metadata\n",
    "metadata = ModelMetadata(\n",
    "    name=\"transfer_learning_classifier\",\n",
    "    version=\"1.0.0\",\n",
    "    model_type=\"transfer_learning\",\n",
    "    accuracy=performance_metrics['accuracy'],\n",
    "    training_date=datetime.now().isoformat(),\n",
    "    model_path=model_save_path,\n",
    "    config=CONFIG,\n",
    "    performance_metrics={\n",
    "        'validation_accuracy': val_accuracy,\n",
    "        'validation_loss': val_loss,\n",
    "        'mean_confidence': performance_metrics['mean_confidence'],\n",
    "        'std_confidence': performance_metrics['std_confidence']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Register the model\n",
    "registry.register_model(metadata)\n",
    "print(\"Model registered successfully in the model registry.\")\n",
    "\n",
    "# Save configuration\n",
    "config_path = '../models/transfer_learning_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "print(f\"Configuration saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Transfer Learning Development Summary ===\")\n",
    "print(f\"Base Model: {CONFIG['base_model']}\")\n",
    "print(f\"Training Strategy: Two-phase (frozen + fine-tuning)\")\n",
    "# Get final validation accuracy from model evaluation (if available)\n",
    "try:\n",
    "    print(f\"Final Validation Accuracy: {val_accuracy:.4f}\")\n",
    "except NameError:\n",
    "    print(\"Final Validation Accuracy: Run evaluation cell to get accuracy\")\n",
    "print(f\"Model Parameters: {model.count_params():,}\")\n",
    "print(f\"Total Classes: {len(class_names)}\")\n",
    "print(f\"Training Images: {n_train}\")\n",
    "print(f\"Validation Images: {n_val}\")\n",
    "\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"- Memory-efficient unified classification dataset with {len(class_names)} classes\")\n",
    "print(f\"- Classes include: {', '.join(class_names[:5])}{'...' if len(class_names) > 5 else ''}\")\n",
    "print(f\"- Images resized to {CONFIG['input_shape'][:2]} for transfer learning\")\n",
    "print(f\"- Images loaded on-demand to save memory\")\n",
    "\n",
    "print(f\"\\nKey Features:\")\n",
    "print(\"- Pre-trained ImageNet weights (ResNet50)\")\n",
    "print(\"- Custom classification head\")\n",
    "print(\"- Two-phase training strategy\")\n",
    "print(\"- Memory-efficient data loading with tf.data\")\n",
    "print(\"- Comprehensive evaluation metrics\")\n",
    "print(\"- Compatible with unified dataset\")\n",
    "\n",
    "print(f\"\\nModel Integration:\")\n",
    "print(\"- Implements BaseImageClassifier interface\")\n",
    "print(\"- Registered in ModelRegistry\")\n",
    "print(\"- Ready for ensemble integration\")\n",
    "print(\"- Compatible with API deployment\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(\"1. Experiment with different pre-trained models\")\n",
    "print(\"2. Optimize hyperparameters\")\n",
    "print(\"3. Implement model ensembling\")\n",
    "print(\"4. Deploy to production API\")\n",
    "print(\"5. Monitor model performance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
